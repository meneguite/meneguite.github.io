{"pages":[{"title":"Sobre","text":"Mais sobre mim","link":"/about/index.html"}],"posts":[{"title":"OAuth","text":"O OAuth é um padrão aberto para autorização, responsável por fornecer um método para acessar os recursos do servidor em nome de seu proprietário, além de fornecer também um processo para que os usuários possam autorizar o acesso de terceiros aos seus recursos, sem compartilhar suas credenciais. O OAuth permite a aplicações acessar dados de um usuário de forma segura, sem que para isso o usuário necessite disponibilizar suas senhas. HistóriaO OAuth teve seu início por volta de novembro de 2006, quando Blaine Cook foi trabalhar na implementação Twitter OpenID e ao buscar uma maneira de usar o OpenID em conjunto com a API do Twitter se juntou a outros que possuíam necessidades similares, como Chris Messina, David Recordon e Larry Halff, analisaram as funcionalidades do OpenID e soluções práticas de outras empresas, e chegaram a conclusão de que não havia nenhum padrão aberto para a delegação de acesso à API. Em julho de 2007, agora com o apoio do Google, foi elaborada uma especificação inicial e o projeto foi aberto a qualquer pessoa interessada em contribuir. Em 3 de Outubro de 2007, a versão final do Núcleo OAuth 1.0 foi lançada. A primeira versão do OAuth foi projetada para lidar com autorização de aplicações web cliente-servidor, não definindo como lidar com autorização em aplicações móveis, desktop, javascript ou extensões do navegador por exemplo. Assim essas aplicações ao implementar o OAuth 1.0, normalmente possuíam métodos inconsistentes e muitas vezes de qualidade inferior, visto que eram obrigados a adaptar o protocolo a sua necessidade. Com o lançamento da versão 2.0 do protocolo, essa limitação foi eliminada, visto que já foi projetada para trabalhar com todos esses tipos de aplicações de forma nativa. Porque usar uma OAuth ao invés de senhas?Para Boyd(2012) existem várias razões para se utilizar OAuth, sendo as principais: Confiança: O usuário pode não confiar em fornecer a senha a sua aplicação. Acesso e risco além do necessário: Ao fornecer sua senha a uma aplicação, o usuário disponibiliza não somente os dados necessários para ela, mais sim todos os dados que estão vinculados a aquela conta. Assim quando o usuário confia sua senha, cabe à mesma armazená-las de forma segura, criando mecanismos com o objetivo de evitar o vazamento dessa informação, o que pode gerar um custo desnecessário para a empresa. Alteração de Senha: Ao alterar a senha de uma conta, todas as aplicações vinculadas a ela irão parar de funcionar, fazendo que o usuário tenha que acessar aplicação a aplicação para inserir a nova senha. Revogação: A única maneira de revogar o acesso à aplicação será o usuário alterar suas senhas, o que também revoga o acesso a todos os outros aplicativos que o mesmo tenha concedido essa informação. Papéis no OAuthExistem vários componentes chaves que compõe os fluxos do protocolo OAuth. São eles: Servidor de recursos : O servidor que hospeda os recursos de propriedade do usuário que estão protegidos por OAuth. Este é normalmente um provedor de API que mantém e protege os dados, como fotos, vídeos, calendários e contatos. Proprietário dos recursos: Normalmente, o usuário de um aplicativo, o proprietário do recurso tem a capacidade de conceder acesso aos seus próprios dados hospedado no servidor de recursos. Cliente: É aquele que envia uma requisição à API, solicitando um recurso protegido em nome do proprietário. Servidor de autorização: O servidor responsável por receber a autorização de proprietário do recurso e conceder tokens de acesso para o cliente poder acessar, em nome do proprietário, os recursos protegidos hospedados em um servidor de recursos. Fornecedores de API menores podem usar o mesmo aplicativo e URL, tanto para o servidor de autorização, quanto para o servidor de recursos. Perfis de Clientes no OAuth 2.0Existem três perfis de clientes implementados no OAuth 2.0, são eles: Aplicação executada pelo servidor (server-side), Aplicação executada pelo navegador de internet do cliente (client-side) e Aplicações nativas Aplicação executada pelo servidor (server-side)Para esse perfil a aplicação web é acessada por um usuário, proprietário do recurso, e ao ser consultada faz as chamadas da API apropriadas usando uma linguagem de programação do lado do servidor, não disponibilizando ao usuário o acesso aos tokens OAuth emitidos pelo servidor de autorização . Na ilustração abaixo é demonstrado o fluxo de uma aplicação que busca autorização pelo lado do servidor. Aplicação executada pelo servidor (server-side) Aplicação executada pelo navegador do cliente (client-side)Nesse perfil o acesso a API é efetuada diretamente no cliente, por intermédio de seu navegador, onde o mesmo tem acesso ao código do aplicativo e suas solicitações. Dessa forma o aplicativo pode ser distribuído em linguagem javascript, ou em uma extensão do navegador ou ainda usando uma tecnologia plug-in como o Flash. Esse perfil, por rodar em um ambiente com pouco ou nenhum controle, possui fragilidades as quais fazem com que, por medida de segurança, alguns provedores não permitam o armazenamento de credenciais para clientes que utilizam esse perfil. Aplicação executada pelo cliente (client-side) Aplicações nativasAplicações com esse perfil são desenvolvidas para rodar como um aplicativo nativo em diversos dispositivos como computadores, smartphones ou tablets. Podendo assim aproveitar de todos os recursos do sistema operacional ao qual o dispositivo disponibiliza. Essas aplicações possuem características muito similares às executadas pelos navegadores de internet, como por exemplo, o fato de não ser um ambiente confiável para o armazenamento de credenciais. Porém como destaca Boyd(2012), uma vez que o aplicativo é instalado, pode não ter acesso a todos os recursos, como teria em um navegador de internet. Fluxo de AutorizaçãoCada um dos perfis citados anteriormente precisa fazer uso de um protocolo adequado para a obtenção de autorização do proprietário do recurso de acesso a seus dados. Em seu núcleo, o protocolo OAuth 2.0 define quatro principais mecanismos para a obtenção dessa autorização, além de também definir um de extensão para permitir tipos de autorização adicionais . Concessão de código de autorizaçãoEsse tipo de concessão é mais adequado para aplicações web que utilizam o perfil server-side. Depois que o proprietário do recurso autorizou o acesso a seus dados, eles são redirecionados de volta para a aplicação web com um código de autorização como um parâmetro de consulta na URL, sendo que este código deve ser trocado por um token de acesso pelo aplicativo cliente. Esta troca é feita de servidor para servidor e requer o client_id e client_secret, impedindo até mesmo o proprietário do recurso de obter o token de acesso. Este tipo de concessão também permite o acesso de longa duração para uma API usando fichas de atualização. Uma representação desse fluxo é demonstrada na ilustração abaixo, onde podemos visualizar a solicitação do recurso pelo cliente para o proprietário do recurso, que responde com uma concessão de autorização gerada pelo servidor de autorização, logo após a concessão de autorização é enviada ao servidor de autorização, que responde com um token de acesso que é usado para acessar o servidor de recursos e obter os dados aos quais obteve autorização prévia. Fluxo de Código de Autorização Concessão implícitaAs concessões implícitas estão disponíveis como um método simplificado de geração do token de acesso para clientes públicos baseados em navegadores, pois ao invés de gerar uma concessão intermediária, como acontece no fluxo de Código de autorização, o token de acesso é emitido diretamente ao cliente depois de autenticar o proprietário do recurso. As etapas ilustradas na imagem abaixo demonstram o fluxo de uma concessão implícita, onde : 1ª Etapa: O aplicativo solicita ao usuário a permissão de acesso a API do Facebook. 2ª Etapa: O usuário se conecta ao servidor de recursos por meio de uma URL que inclui informações sobre o aplicativo que está tentando acessar a API, como por exemplo, o client_id, e fornece um nome de usuário e senha para fazer login no facebook. 3ª Etapa: Com login efetuado com sucesso, o facebook concede ao aplicativo um token de acesso. 4ª Etapa: O aplicativo pode obter acesso aos recursos do usuário ao se utilizar do token recebido na etapa anterior. Fluxo de concessão implícita Fonte: KULP (2013) Concessão com usuário e senhaEste tipo de concessão consiste em ao obter um nome de usuário do proprietário de um recurso e uma senha para que esses dados possam ser trocados por um token de acesso OAuth. Este tipo de concessão deve ser usado somente para clientes altamente confiáveis, tais como aplicações móveis escritas pelo provedor da API, visto que a senha do usuário é exposta ao cliente, que não necessita de armazená-la no dispositivo. Após a autenticação inicial, apenas o token OAuth precisa de ser armazenado. Como a senha não é armazenada, o usuário pode revogar o acesso ao aplicativo sem alterar a senha, e o token é limitado a um conjunto de dados previamente autorizado pelo proprietário dos recursos, de modo que este tipo de concessão ainda proporciona maior segurança sobre a autenticação de usuário / senha tradicional. Concessão de credenciais para o clienteA concessão de credenciais de um cliente permite que um aplicativo possa obter um token de acesso a recurso de propriedade do cliente quando a autorização foi previamente definida com o servidor de autorização. Sendo esse tipo de concessão adequada para aplicações que precisam acessar APIs, tais como serviços de armazenamento ou banco de dados em nome da aplicação ou do fornecedor da aplicação e não em nome de um usuário específico. Concessão de credenciais para um dispositivoO concessão de credenciais para um dispositivo foi criada para permitir que o protocolo OAuth possa ser usado em dispositivos mais limitados, sem os recursos necessários para obter uma autorização por si só. Essa consiste em iniciar o fluxo no dispositivo e então utilizar dispositivo auxiliar, como um computador, por exemplo, para acessar um site e aprovar o acesso, digitando um código de autorização exibida no dispositivo. BibliografiaBOYD, R. Getting Started with OAuth 2.0. Sebastopol: O’Reilly Media, 2012. KULP, T. Access Online Services with the Windows Runtime and OAuth. MSDN Magazine, 2014. Disponivel em:&lt;http://msdn.microsoft.com/pt-br/magazine/jj883954(en-us).aspx&gt;. Acesso em: Março 2014.","link":"/2014/04/14/oauth/"},{"title":"Replicação com OpenLdap","text":"Nesse artigo iremos demonstrar o funcionamento do sistema de replicação com o OpenLDAP. Replicação é a manutenção de uma cópia, seja parcial ou total, dos dados em outros servidores. Este é um método muito utilizado quando trabalhamos em um ambiente corporativo, onde temos a necessidade de alta disponibilidade nos serviços. Base Teórica Sobre Replicação no OpenLdapReplicação é a manutenção de uma cópia, seja parcial ou total, dos dados em outros servidores. Esse é um método muito utilizado quando trabalhamos em um ambiente corporativo, onde temos a necessidade de alta disponibilidade nos serviços. No OpenLDAP existem duas técnicas distintas de replicação as quais são o slurpd e o syncrepl, sendo que a slurpd é uma técnica mais antiga a qual foi descontinuada na versão 2.4 do OpenLDAP por possuir uma série de limitações que foram supridas por sua sucessora a syncrepl. As principais limitações eram a replicação parcial de uma base e a utilização de mais de um servidor master. Quando trabalhamos com o syncrepl, o servidor LDAP pode assumir duas posturas quanto à replicação: master ou slave. Quando master, o servidor assume todas as funções de um servidor OpenLDAP, desde a inserção, atualização e consulta dos dados. Porém, quando slave, o servidor só aceitará execuções de consultas em sua base de dados quando originadas de máquinas clientes. Assim, para que qualquer informação seja alterada, a solicitação deverá ser feita ao servidor Master, o qual se encarregará de replicar as alterações para seus servidores slaves. Replicação Master x SlaveNesse método de replicação temos um único servidor master, o qual tem seu conteúdo replicado em todos os seus servidores slaves. Nesses servidores slaves não ocorre entrada de dados, apenas replicam passivamente os dados de um servidor principal. Esse método de replicação é o mais comum e atende a grande parte das demandas de servidores LDAP. Replicação Master x Master (Multimaster)Replicação multimaster é o método de replicação mais recente no OpenLDAP, porém já implementado no Active Directory desde suas primeiras versões. Esta é uma replicação que atende a demandas muito singulares e apesar de funcionar muito bem, ainda é pouco difundida e aplicada. Esta replicação consiste na utilização de dois ou mais servidores masters, independentes, os quais possuem todas as suas funcionalidades ativadas, porém é criado por parte dos servidores um controle interno para que seja possível manter a integridade dos dados, algo que era muito mais simples de controlar quando existia uma única entrada de dados. Diretórios DistribuídosUtilizando o OpenLDAP com o método de replicação syncrepl podemos não somente criar uma cópia da árvore de diretórios em outros servidores, mas também criar políticas de replicação e replicar a cada servidor somente aquilo que se pretende que o mesmo tenha acesso, assim por exemplo, quando temos uma empresa com sede em Minas Gerais e filial em São Paulo, a mesma não precisa replicar toda sua base para a filial, apenas aqueles usuários que a pertençam, evitando assim um tráfego desnecessário na rede além de aumentar a segurança. Exemplo de ConfiguraçãoBom, abaixo iremos apresentar um exemplo de configuração de replicação para o OpenLDAP fazendo uso da estrutura Master x Slave. Consideraremos que já possuímos dois servidores configurados com o servidor OpenLDAP instalado e funcionando, assim apenas demonstraremos os parâmetros relacionados as replicações entre os servidores. A distribuição Linux utilizada foi Debian 5 e o OpenLDAP na versão 2.4.9. Configuração Master X SlaveConfigurando o Servidor MasterInicialmente o arquivo a ser alterado é o arquivo principal de configuração do OpenLDAP o /etc/ldap/slapd.conf. Alterando o parâmetro modulepath para syncprov e inserindo as seguintes linhas logo abaixo da opção “index”: 123overlay syncprovsyncprov-checkpoint 100 10syncprov-sessionlog 100 Devemos Inserir também junto a ACL responsável pelos atributos de senha, userPassword e shadowLastChange a seguinte linha: 1by dn=\"cn=replicator,dc=dominio,dc=com,dc=br\" read Assim inserimos a permissão para o usuário replicador, ao qual deve ser criado anteriormente para ler os campos de senha dos usuários, pois sem o acesso a leitura ele não conseguiria efetuar a replicação completa da base. Neste ponto foi reiniciado o servidor com o comando: 1/etc/init.d/slapd restart Configurando o Servidor SlaveIniciaremos a configuração do servidor slave editando o arquivo de configuração /etc/ldap/slapd.conf e removendo o caractere “#” antes da linha “rootdn”, e logo após inserimos as seguintes linhas abaixo de “index”: 12345678910111213syncrepl rid=1provider=ldap:// servidor1.dominio.com.br:389type=refreshAndPersistretry=\"5 + 5 +\"interval=00:00:00:10searchbase=\"dc=dominio,dc=com,dc=br\"filter=\"(objectClass=*)\"scope=subattrs=\"*\"schemachecking=on 41bindmethod=simplebinddn=\"cn=replicator,dc=dominio,dc=com,dc=br\"credentials=senha_do_usuario_replicator Após os procedimentos acima citados, paramos o servidor OpenLDAP, removemos os diretórios e novamente iniciamos o servidor. Para isso utilizamos as seguintes linhas de comando: 123/etc/init.d/slapd stoprm /var/lib/ldap/*/etc/init.d/slapd start Assim temos agora os dois servidores configurados, sendo que os dados do servidor1.dominio.com.br estão sendo replicados para o servidor2.dominio.com.br, porém o mesmo não tem autoridade sobre os dados fazendo com que para alterarmos qualquer informação na base, teremos que fazer isso no servidor1.dominio.com.br e essa atualização será replicada de imediato para o servidor slave.","link":"/2010/03/04/replicacao-com-openldap/"},{"title":"GoLang, desbravando uma linguagem de programação - Parte 1","text":"Image from: http://kirael-art.deviantart.com/art/Go-lang-Mascot-458285682 Já faz muito tempo que não escrevo um artigo, mas já fazia algum tempo que queria voltar a escreve-los. Hoje resolvi aprender uma nova linguagem de programação e também documentar esse processo na esperança de talvez poder ajudar alguém, não só aprender a própria linguagem Go, como qualquer outra linguagem de programação. Esta série de artigos não tem a pretensão de ser um guia definitivo, ou um super manual da linguagem, mas sim descrever como eu aprendo uma nova linguagem, pontos importantes e minhas impressões da mesma. Bom então vamos lá… O que é GoGo é um projeto de código aberto, multiplataforma, desenvolvido e mantido pelo time do Google além de outros contribuidores e distribuído com a licença BSD. O projeto nasce com o objetivo de tornar seus programadores mais produtivos e ser uma linguagem expressiva, concisa, limpa e eficiente. Bom, isso é o que está descrito em sua documentação, por hora vamos apenas acreditar :) . Podemos encontrar mais informações sobre o projeto aqui. Leia a documentação da linguagem, esse é o primeiro passo para entender o que é e o que esperar ao aprendê-la. Saber os objetivos do ecossistema é fundamental para entender onde é possível e viável usar a linguagem. Perceba que entender isso é como conhecer qualquer ferramenta que se proponha a usar, tentar apertar um parafuso com um martelo tende a não dar muito certo 😁. E porque os objetivos do ecossistema e não da linguagem? Bom, esse é um erro muito comum para iniciantes. É fundamental perceber que o que limita o uso de uma linguagem, não é sua sintaxe. No geral a linguagem e seu ecossistema nascem e evoluem em um mesmo objetivo, porém é possível que isso mude com o tempo. O JavaScript é um exemplo muito claro disso, foi por muito tempo uma linguagem limitada ao browser, com aplicabilidade apenas em sistemas web, porém esse cenário mudou completamente após a chegada do NodeJs, hoje vemos JavaScript para todos os lados, seja em servidores ou até em pequenos dispositivos embarcados. Uma impressão inicial, e que me agrada bastante, é que logo no inicio de sua documentação, Go deixa claro ter seu foco em aproveitar o máximo de maquinas multicore e em rede, esse objetivo é super importante, porque iniciar em um ecossistema que não trabalha bem com escalabilidade seja ela vertical ou horizontal é hoje na minha visão uma perda de tempo na grande maiorias dos casos. Outro ponto que me interessou bastante é o fato de Go ser é uma linguagem com tipagem estática e compilada, o que destoa bastante das linguagens que venho trabalhando nos últimos tempos, e pode ser um trufo interessante em novos projetos. Entendendo a sintaxe básica da linguagemAo iniciar em uma linguagem é importante entender sua sintaxe básica e sempre que possível suas origens. Quase todas as linguagens modernas não começaram do zero, foram inspiradas em linguagens já existentes. Traçar sua ancestralidade ajuda a criar familiaridade mais rapidamente caso já possua domínio de outra com ancestral comum. Por exemplo, linguagens baseadas em C tendem a ter características em comum e quanto antes perceber isso, mais rápido vai se sentir a vontade na nova linguagem. Go é um exemplo disso, muito de sua sintaxe é semelhante ao C, com pequenas mudanças como a falta de parênteses em volta de estruturas como for e if por exemplo. É super tranquilo se adaptar as mudanças.. abaixo um exemplo de código Go. 1234567package mainimport \"fmt\"func main() { fmt.Println(\"Hello World\")} Todos os detalhes da Especificação da linguagem podem ser encontrados aqui, é sempre bom dar uma olhada, mas essa documentação pode e deve ser consultada sempre que alguma dúvida aparecer. A documentação da especificação do Go é muito detalhada e bem escrita, eu realmente me senti confortável ao fazer uma leitura rápida da mesma. Configurando o ambiente de desenvolvimentoPara iniciarmos em uma nova linguagem, quase sempre, precisamos de um ambiente bem configurado de forma possibilitar a fluides no aprendizado. Hoje dificilmente instalo qualquer ambiente diretamente em minha maquina, por algum tempo usei o Vagrant para isso, porém hoje tenho trabalho quase exclusivamente com o Docker, ele possibilita testar e descartar sempre que não uso mais qualquer ambiente, deixando minha máquina sempre livre. Outro ponto importante é que consigo reproduzir o exato ambiente ao qual o sistema vai rodar quando em produção, evitando problemas de compatibilidade no momento em que coloco o sistema no ar. O intuito desse artigo não está em usar o Docker, assim apenas me limitarei a mostrar os passos feitos e não a explica-los a fundo, mas sugiro fortemente que tire um tempinho para dominar ele, provavelmente vai ser um divisor de água no seu dia a dia como desenvolvedor. Para iniciarmos o projeto criei um arquivo chamado “docker-compose.yml” na raiz do projeto com o seguinte conteúdo: 1234567version: \"2\"services: app: image: golang:1.9-stretch volumes: - ./:/go/src/learning-go-lang working_dir: /go/src/learning-go-lang Esse arquivo é responsável por inicializar com as configurações adequadas o contêiner docker para a aplicação. Posteriormente criei um arquivo de nome de “main.go” ainda na raiz do projeto com o seguinte conteúdo: 1234567package mainimport \"fmt\"func main() { fmt.Println(\"Hello World\")} Tudo finalizado e configurado, basta executar o comando “docker-compose run app go run main.go”, esse comando ao ser rodado a primeira vez baixa o container configurado no arquivo “docker-compose.yml” e posteriormente executa o arquivo main.go. (Esse processo pode demorar um pouco a primeira vez, dependendo de sua conexão com a internet, porém nas demais o processo é super rápido). Assim, após finalizado o processo, todo o ambiente configurado e pronto para começarmos os trabalhos devera gerar o tão esperado Hello World em sua última linha. Próximos CapítulosPara continuação dessa série alguns dos próximos passos serão, entender melhor como funciona o sistema de Gerenciamento de Dependências do Go, todos os projetos, quase sem exceção dependem de ferramentas externas, Go parece ser bem completo, mas ter e entender como funciona seu gestor de dependência é fundamental. O passo seguinte será identificar se existem e quanto maduro são os frameworks disponíveis para a linguagem, por fim minha ideia é criar uma API simples usando os principais conceitos da linguagem e posteriormente a levar para produção para que seja possível perceber as dificuldades envolvidas no processo. Bom por hoje é isso, em breve seguimos.. Click aqui para acessar a segunda parte do artigo!! Ou Click aqui para acessar a terceira e ultima parte do artigo!!","link":"/2017/10/01/golang-desbravando-uma-linguagem-de-programacao-parte-1/"},{"title":"GoLang, desbravando uma linguagem de programação - Parte 2","text":"Este artigo é a segunda parte de uma sequência de artigos sobre meu processo de aprendizado de uma nova linguagem de programação, nessa série tenho o objetivo de documentar esse processo na esperança de talvez poder ajudar alguém, não só aprender a própria linguagem Go, como qualquer outra linguagem de programação. A parte 1 dessa série pode ser lida aqui. Antes de começarmosUm ponto que venho percebendo durante meus estudos em Go, é que é muito fácil perceber o porque quase sempre se usa o termo Golang e não somente Go como inicialmente concebido. É inviável achar qualquer coisa em um buscador usando apenas Go, assim provavelmente irei me referir algumas vezes ao Go usando este termo e acabei por alterar os títulos dos artigos para facilitar a indexação nos mecanismos de busca. Outro adendo importante que gostaria colocar é um utilitário web disponibilizado pela própria Google para testarmos a linguagem, sendo possível testar a Golang mesmo sem ter nada instalado localmente. Esse utilitário denominado The Go Playground pode ser encontrado em https://play.golang.org e realmente ajuda muito nos testes iniciais da linguagem. https://play.golang.org Mas afinal quem usa Go?Bom, além do próprio Google, outras grandes empresas estão investindo no Go, com a Sendgrid, Globo.com, Mercado Livre, Magazine Luiza, Walmart além de várias outras gigantes no mercado. Um caso de uso bem interessante é o da sendgrid que pode ser lindo aqui onde seu Co-founder Tim Jenkins descreve os motivos da escolha da linguagem e o mais importante, os problemas encontrados nessa adoção e como eles superaram eles. Outro caso de uso interessante é o do Globo.com que pode ser visto aqui, onde o analista Vinicius Pacheco descreve como saíram de 200 para 19 mil cadastros por segundo usando microservices e Go. Quando usar o Go ?Sinto muito, mas Golang não é a bala de prata que estava esperando 😞, pode parecer uma brincadeira, mas ainda hoje, mesmo com tantos artigos publicados sobre o assunto, não falta desenvolvedores em busca de uma linguagem/ecossistema que atenda todas as demandas possíveis da melhor forma. Para esses, eu tenho uma triste notícia, isso provavelmente nunca vai acontecer. Entender o que cada ecossistema de linguagem tem de melhor, e onde é interessante ou não usá-la é fundamental para que possamos prover sempre a melhor solução para cada problema. Go tem objetivos muito claros, se precisa de um sistema multiplataforma, altamente escalável, com desempenho similar outras linguagens compiladas como C, que trabalhem com programação concorrente de forma nativa e de maneira altamente otimizada, então meu amigo, Go é pra você 😉. Se esses objetivos não se alinham com seu projeto, então esqueça, provavelmente Go não é a melhor opção. Para se manter fiel a seus objetivos, Golang tende a ser simplista ao extremo, e não implementa, ou implementa de forma alternativa vários mecanismos hoje comum em quase todas as linguagens de alto nível. Para citar alguns desses pontos: Não tem classes; Não tem herança; Não é possível fazer overload de métodos; Não existe sistemas de Exceptions (try/catch); 😱 Não tem conversões numéricas implícitas; Não possui Operadores Ternários; 😢 Isso mesmo, pode parecer estranho, eu mesmo ainda não me acostumei com isso, mas Go tem um suporte a orientação a objetos extremamente peculiar. Mas não desista ainda, por exemplo, para suprir a necessidade criada pela falta de classes, Go disponibiliza as Structs, que irão permitir trabalhar de forma similar a que estamos acostumados com as classes. Iremos ver isso com mais calma em um próximo artigo. O tratamento de erros em Go vem se destacando em meus estudos como um grande ponto fraco do ecossistema, e é comum ver que a maioria dos desenvolvedores usam a capacidade de Golang de retornar vários valores em uma mesma função para fazer um tratamento que pra mim ainda parece um pouco primitivo. Abaixo um exemplo de tratamento de erro: 12345f, err := os.Open(\"filename.ext\")if err != nil { log.Fatal(err)} Para Go os erros não podem ser delegados, são considerados com parte fundamental do código. Um texto interessante que fala sobre isso, escrito por Elton Minetto pode ser lido aqui. Gerenciamento de dependênciasNos dias de hoje dificilmente conseguiremos trabalhar em um sistema completamente sem dependências externas, até por que não faria sentido reinventar a roda a cada novo projeto. Go vem inserido em um ecossistema muito rico de funcionalidades, traz já em sua base, funcionalidades como testes por exemplo, que na maioria das linguagens ficam a cargo de pacotes de terceiros. Porém isso não acontece com o sistema de gerência de dependências, o que é uma pena, considerando o grau de importância de um sistema realmente confiável para esse fim. Não tendo uma ferramenta padrão para isso, acabou por ficar por conta da comunidade o desenvolvimento de uma ferramenta para esse fim, e assim fizeram, várias opções foram surgindo, cada uma com sua abordagem e incompatíveis entre-se, dessas aparentemente a mais relevante é a Glide, que parece ser entre todas a mais madura e concisa. Porém esse problema parece estar chegando ao fim, enquanto escrevo esse artigo existe uma iniciativa da própria comunidade em criar uma ferramenta de nome dep que ainda é um experimento oficial, mas tende a se tornar a ferramenta oficial para esse fim. Nessa série já vamos iniciar com ele, visto que já é considerado “safe for production use” apesar de estar em processo acelerado de desenvolvimento. O processo de instalação do dep é extremamente simples, mas varia de acordo com seu ambiente. Para essa série, irei continuar usando o docker, mas a imagem padrão não mais vai nos atender, assim vamos fazer uma build customizada já com o dep disponível. Para isso, vamos criar um novo arquivo config/docker/go/Dockerfile, que conterá a receita para criar a imagem do docker. 1234# Arquivo config/docker/go/DockerfileFROM golang:1.9-stretchENV TZ America/Sao_PauloRUN go get -u github.com/golang/dep/cmd/dep E posteriormente vamos precisar alterar nosso arquivo “docker-compose.yml” para que ele use essa imagem, ficando assim: 12345678version: \"2\"services: app: build: ./config/docker/go volumes: - ./:/go/src/learning-go-lang working_dir: /go/src/learning-go-lang Para executar o build executamos o comando “docker-compose build” e tudo correndo bem podemos voltar a executar o comando “docker-compose run app go run main.go” e teremos novamente impresso nosso querido Hello World. Próximos CapítulosO passo seguinte será identificar se existem e quanto maduro são os frameworks disponíveis para a linguagem, por fim minha ideia é criar uma API simples usando os principais conceitos da linguagem e posteriormente a levar para produção para que seja possível perceber as dificuldades envolvidas no processo. Obs.: Os códigos produzidos durante o projeto podem ser acompanhados usando o repositório hospedado no github -> https://github.com/meneguite/golang-learning Bom por hoje é isso, em breve seguimos.. Click aqui para acessar a terceira e ultima parte do artigo!!","link":"/2017/10/08/golang-desbravando-uma-linguagem-de-programacao-parte-2/"},{"title":"Adapter Pattern - Design Patterns com Typescript","text":"Nesse artigo pretendo apresentar de uma forma simplificada o conceito de Adapter Pattern, que é um padrão de projeto extremamente útil no dia a dia para qualquer desenvolvedor. Sua função é converter uma interface de uma classe para outra esperada pelo cliente, possibilitando que classes com interfaces incompatíveis trabalhem juntas. Para simplificar o conceito, imagine que tenha um novo notebook que comprou fora do brasil, seu plugue de energia vem no padrão europeu, infelizmente incompatível com as tomadas brasileiras, para resolver esse problema é simples, vai precisar de um adaptador. Bom, o Adapter Pattern tem exatamente a mesma função, conectar dois pontos incompatíveis entre si. Com o conceito entendido, hora de colocar isso no código em um exemplo mais real encontrado no dia a dia de muitos desenvolvedores, de forma muito simplificada é claro, mas que já vai nos dar uma boa ideia de como expandirmos e aplicarmos isso no nosso dia a dia. Considere que tem hoje um sistema de log que salva em um arquivo local todas as informações geradas pela aplicação, e que esse possui a seguinte interface e implementação: 12345678910interface Logger { info(message: string): Promise;}class FileLogger implements Logger { public async info(message: string): Promise { console.info(message); console.info('This Message was saved with FileLogger'); }} Essa mesma implementação é usada por todo o sistema e sempre gerado um log o mesmo é tratado por essa classe FileLogger como no exemplo abaixo: 12345678910111213141516171819class NotificationService { protected logger: Logger; constructor (logger: Logger) { this.logger = logger; } public async send(message: string): Promise { //... Implementation await this.logger.info(`Notification sended: ${message}`); }}// Inicialização com o FileLogger(async () => { const fileLogger = new FileLogger(); const notificationService = new NotificationService(fileLogger); await notificationService.send('My notification');})(); Ao rodar esse código é esperado algo como: 123# tsc && node adapter/index.jsNotification sended: FoiiThis Message was saved with FileLogger Porém agora precisamos usar uma nova forma de salvar os logs, pois com o crescimento de nossa aplicação, salvar em disco não é mais uma alternativa, assim precisamos usar uma implementação que responde pela seguinte interface e implementação: 12345678910interface CloudLogger { sendToServer(message: string, type: string): Promise;}class AwsLogger implements CloudLogger { public async sendToServer(message: string, type: string): Promise { console.info(message); console.info('This Message was saved with AwsLogger'); }} Ou seja, para que possamos usar essa nova classe precisaríamos refatorar todo nosso código para usar o novo formato de envio de log, ou usar um adapter, o que convenhamos, parece uma opção bem melhor 🙂.Esse adapter poderia ser algo como: 1234567891011class CloudLoggerAdapter implements Logger { protected cloudLogger: CloudLogger; constructor (cloudLogger: CloudLogger) { this.cloudLogger = cloudLogger; } public async info(message: string): Promise { await this.cloudLogger.sendToServer(message, 'info'); }} Com esse adapter poderíamos enviar uma instância do mesmo e manter todo o código de nossas implementações intacto, apenas alterando a inicialização como pode ser visto no exemplo abaixo: 1234567// Inicialização com o AwsLogger(async () => { const awsLogger = new AwsLogger(); const cloudLoggerAdapter = new CloudLoggerAdapter(awsLogger); const notificationService = new NotificationService(cloudLoggerAdapter); await notificationService.send('My notification');})(); Assim teríamos uma saída esperada: 123# tsc && node adapter/index.jsNotification sended: FoiiThis Message was saved with AwsLogger Como podem perceber esse é um padrão extremamente útil e é fundamental para qualquer desenvolvedor o entendimento desse padrão. Bom por hoje é isso, mais em breve pretendo escrever outros artigos como esse, com intuito de simplificar e trazer para o nosso dia a dia os padrões de projetos ou design patterns mais comuns aplicados usando o typescript. O Código completo para esse implementação pode ser encontrado em: https://github.com/meneguite/typescript-design-patterns/blob/master/adapter/index.ts Outros padrões de Projetos: Adapter Pattern Observer Pattern","link":"/2019/06/20/design-patterns-com-typescript-adapter/"},{"title":"GoLang, desbravando uma linguagem de programação - Parte 3 (Final)","text":"Este artigo é a terceira e ultima parte de uma sequência de artigos sobre meu processo de aprendizado de uma nova linguagem de programação, nessa série tenho o objetivo de documentar esse processo na esperança de talvez poder ajudar alguém, não só aprender a própria linguagem Go, como qualquer outra linguagem de programação. A parte 1 dessa série pode ser lida aqui e a parte 2 aqui. Antes de começarmosApós algum tempo aprendendo Golang, confesso que configurar uma ambiente funcional com go e docker não foi das tarefas mais simples. Após muita leitura e varias tentativas e erros acabei por adotar um modelo hibrido, onde uso o docker apenas para testar no ambiente mais próximo ao ambiente de produção, mas para o desenvolvimento acabei por configurar o ambiente local 😒. Assim simplifiquei bem dockercompose.yml e removi o Dockerfile que havia criado no artigo anterior. Frameworks no mundo GoO uso de frameworks em Go é viável, mas nem sempre necessário. Ao adentrar no ecossistema de Go é possível perceber o quão rico ele é, hoje mesmo sendo uma linguagem relativamente nova, é possível perceber um número enorme de opções de bibliotecas que abrangem a grande maioria das necessidades que um desenvolvedor pode ter em seu dia a dia, podendo assim tecer sua própria solução sem maiores dificuldades. Dias atrás tive acesso a um repositório que tem o intuito de catalogar e classificar os principais pacotes disponíveis para uso, pode ver ele aqui. Ele é realmente completo, tem dezenas de categorias e diversas opções para cada uma delas, certamente vale uma conferida. Ao acessar https://github.com/avelino/awesome-go#web-frameworks perceberá uma lista considerável com diversos frameworks disponíveis para o desenvolvimento de aplicações web, que será o objetivo do projeto final dessa dessa série. Desses, por indicação da própria comunidade no slack¹ ( que por sinal recomendo muito que todos que tenham interesse na linguagem participe), e baseado também em minhas pesquisas, echo e gin são os dois mais expressivos hoje. O uso de frameworks é controverso para iniciantes em uma linguagem. É fácil perceber como eles podem dar aquele gás para quem ainda está iniciando, mostrando bons caminhos, boas escolhas de design para a aplicação e muito mais. Porém é comum ver eles sendo usados como muleta para desenvolvedores que realmente não aprenderam sobre a linguagem e seu ecossistema. Assim cabe a quem esta iniciando a escolha, ou usa ele como escada ou como muleta, eu prefiro usar como escada, e esse processo me ajuda muito a aprender e me ambientar. Entre os frameworks destacados muitas similaridades são perceptíveis, tanto o echo como o gin tem um foco muito claro em performance, chegando a prometer performance ainda maior que teríamos se usarmos o módulo nativo. E isso faz muito sentido, se um dos maiores diferenciais da linguagem é a alta performance, é natural que os frameworks que compõem seu ecossistema também sigam a mesma linha. Nesse ponto é fundamental que leiam a documentação dos frameworks que parecem mais promissores, com a documentação poderá alinhar se os objetivos e funcionalidades do mesmo alinham com as do projeto. Implementando uma API em GolangBom agora é hora da parte legal, bora colocar a mão na massa e criar uma API restfull para entender melhor os desafios e as possibilidades que o ecossistema de Go disponibiliza. O objetivo da api é simular um microservice de autenticação extremamente simplista e sem nenhuma pretensão de levá-lo para produção. As principais features esperadas para essa implementação são: Cadastro de um novo usuário Atualização de usuário já cadastrado Remoção de usuário existente Listagem com todos os usuários cadastrados Validar um usuário por seu nome de usuário e senha Para essa implementação resolvi usar o echo como suporte para me prover as funcionalidades básicas para desenvolver essa API. Iniciando um novo projetoPara iniciar um novo projeto go, já dentro da nova pasta crie um arquivo de nome main.go e inclua o seguinte conteúdo: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124package mainimport ( \"net/http\" \"golang.org/x/crypto/bcrypt\" \"github.com/labstack/echo\")type User struct { Email string `json:\"email,omitempty\" form:\"email\"` Password string `json:\"-\" form:\"password\"` Name string `json:\"name,omitempty\" form:\"name\"` Role string `json:\"role,omitempty\" form:\"role\"`}var users []User// Retorna a listagem de todos os usuários cadastradosfunc getAllUsers (c echo.Context) error { return c.JSON(http.StatusOK, users)}// Retorna os dados de um usuáriofunc getUser (c echo.Context) error { email := c.Param(\"user\") user := getUserByEmail(email) return c.JSON(http.StatusOK, user)}// Cadastra um novo usuáriofunc createUser (c echo.Context) error { password := c.FormValue(\"password\") hashPassword, _ := bcrypt.GenerateFromPassword([]byte(password), 10) user := User{Email: c.FormValue(\"email\"), Password: string(hashPassword), Name: c.FormValue(\"name\"), Role: c.FormValue(\"role\")} users = append(users, user) return c.JSON(http.StatusCreated, user)}// Atualiza um usuário existentefunc updateUser (c echo.Context) error { email := c.Param(\"user\") for index, user := range users { if (user.Email == email) { name := c.FormValue(\"name\"); if ( name != \"\") { users[index].Name = name } role := c.FormValue(\"role\"); if ( role != \"\") { users[index].Role = role } password := c.FormValue(\"password\"); if ( password != \"\") { hashPassword, _ := bcrypt.GenerateFromPassword([]byte(password), 10) users[index].Password = string(hashPassword) } return c.JSON(http.StatusOK, users[index]) } } return c.JSONBlob(http.StatusBadRequest, []byte(\"{\\\"status\\\": \\\"error\\\"}\"))}// Exclui um usuário existentefunc deleteUser (c echo.Context) error { email := c.Param(\"user\") for index, user := range users { if (user.Email == email) { users = append(users[:index], users[index+1:]...) return c.JSONBlob(http.StatusOK, []byte(\"{\\\"status\\\": \\\"success\\\"}\")) } } return c.JSONBlob(http.StatusBadRequest, []byte(\"{\\\"status\\\": \\\"error\\\"}\"))}// Faz a autenticação de um usuáriofunc authUser (c echo.Context) error { email := c.FormValue(\"email\") password := c.FormValue(\"password\") user := getUserByEmail(email) err := bcrypt.CompareHashAndPassword([]byte(user.Password), []byte(password)) if (err == nil) { return c.JSONBlob(http.StatusOK, []byte(\"{\\\"status\\\": \\\"success\\\"}\")) } return c.JSONBlob(http.StatusUnauthorized, []byte(\"{\\\"status\\\": \\\"error\\\"}\"))}// Retonar instancia do usuário apartir de um e-mailfunc getUserByEmail(email string) User { var user User; for _, user = range users { if (user.Email == email) { return user } } return user;}func main() { // Inicia usuário password, _ := bcrypt.GenerateFromPassword([]byte(\"123456\"), 10) users = append(users, User{Email:\"ronaldo@test.com\", Password: string(password), Name:\"Ronaldo Meneguite\", Role:\"Admin\"}) router := echo.New() v1 := router.Group(\"/v1\") // User API v1.GET(\"/users\", getAllUsers) v1.GET(\"/users/:user\", getUser) v1.POST(\"/users\", createUser) v1.PUT(\"/users/:user\", updateUser) v1.DELETE(\"/users/:user\", deleteUser) // Auth API v1.POST(\"/auth\", authUser) router.Logger.Fatal(router.Start(\":8080\"))} E execute o seguinte comando: 1dep init Após o processo é esperado que seja criado na pasta corrente uma nova de nome vendor que contém as dependências do projeto, e dois novos arquivos Gopkg.toml e Gopkg.lock, onde o Gopkg.toml será responsável por manter o registros das dependências diretas do projeto e o Gopkg.lock de armazenar as versões de todos os packages instalados atualmente. Feito o processo, já temos nossa primeira API pronta para ser executada. Para isso execute o comando: 1go run main.go Assim feito já teremos nossa API rodando no endereço http://localhost:8080. Como puderam perceber, trabalhar com Golang não tem muitos mistérios, é certamente uma linguagem muito promissora e possui tempos de resposta realmente impressionantes, como pode ser visto abaixo: Levando nossa API a ProduçãoBom com tudo pronto e rodando localmente é hora de ver como seria colocar uma aplicação desenvolvida em Go em produção. Para isso decidi usar a Google Cloud Platform usando o serviço de App Engine. Esse artigo não tem o intuito de mostrar como fazer todo o processo de deploy usando o Google Cloud Platform, assim me limitarei a demostrar os processos feitos dentro do projeto para coloca-lo em produção e descrever minha experiência com o processo. Depois de criar o projeto no console é necessário criar um arquivo de nome app.yaml que deve possuir o seguinte conteúdo: 12runtime: goenv: flex E executar o seguinte comando: 1gcloud app deploy Após alguns minutos já terá uma sua API completamente funcional e em produção, e o que parece magica por tanta simplicidade, na verdade é um casamento perfeito entre o sistema de deploy do Google e sua linguagem. ConclusãoHoje ainda não me vejo conseguindo uma produtividade próxima da que tenho em PHP ou Javascript em Golang, principalmente em projetos monolíticos maiores, mas acho que esse jogo pode mudar muito a medida que os microservices entram em cena. Go parece nascer perfeito para esse fim, é confiável, rápido, simples e altamente escalável, certamente foi uma excelente aquisição para minha “caixa de ferramentas” e espero ter oportunidade de trabalhar diretamente com ela em breve. ¹ Para entrar no slack da comunidade acesse o link https://invite.slack.golangbridge.org. No canal **brazil** vai encontrar muitos brasileiros e pode seguir com seu idioma, porém caso se sinta-se a vontade com o inglês, o canal **general** é ainda mais ativo :)","link":"/2017/11/15/golang-desbravando-uma-linguagem-de-programacao-parte-3/"},{"title":"Publicar um site com Github Pages e CloudFlare","text":"Nesse artigo pretendo apresentar como publicar um site estático com domínio próprio, HTTPS, Cache, proteção DDOS e o melhor, de forma totalmente gratuita usando Github Pages e CloudFlare. Essas instruções são também válidas para aplicações mais complexas como uma SPA (Single Page Application) por exemplo. Já a algum tempo percebo que muitas pessoas usam linguagens dinâmicas para produção de sites estáticos, pela simples motivação de buscar um reaproveitamento do código, já vi e até mesmo já fiz isso. Hoje com o frontend cada vez mais inteligente podemos gerar interfaces extremamente ricas sem ao menos a necessidade de um servidor dinâmico próprio, apenas servindo HTML de forma estática e deixando toda interatividade do lado do cliente. Hoje, até mesmo plataformas como o Wordpress, Joomla e Drupal podem ser substituídas, em alguns casos, por geradores de sites estáticos como Jekyll, Hexo, ou Hugo por exemplo, que a partir de dados dinâmicos podem gerar sites “estáticos” e reduzir consideravelmente o custo com servidores dinâmicos, escalando aplicações de uma forma muito mais simples, com uma performance incrível e um custo de hospedagem mínimo, ou como no exemplo desse artigo, a custo zero. Entenda que não estou de forma alguma dizendo que sites estáticos são a bala de prata para resolver todos os seus problemas, mas sem dúvida ter uma visão sobre eles e perceber o valor dessa abordagem em alguns momentos pode ser um diferencial em sua carreira. Pensando nisso e para atender alguns questionamentos que vinham me fazendo há algum tempo, resolvi escrever este artigo, que pretende demonstrar uma forma muito simples de publicar um site de forma profissional e totalmente gratuita, com domínio próprio, deploy automatizado, cache habilitado, proteção DDOS, certificado HTTPS válido e muitas outras demandas de um site realmente profissional. Para atender essa demanda, vamos nos utilizar de uma composição de ferramentas muito conhecidas no mercado, como o Github Pages e Cloudflare para prover toda a estrutura necessária para disponibilizar nosso site na internet. Para entender todo o conteúdo desse artigo é importante possuir conhecimentos básicos de versionamento usando o Git, e o ter instalado em sua máquina para que possa interagir com o github. Além disso vai precisar de uma conta no Github e uma no CloudFlare, caso não tenha pode providenciar com bastante facilidade, pois são ambas gratuitas e muito úteis para um desenvolvedor. Criando um repositório no Github PagesO primeiro passo para iniciarmos, é criar um repositório ao qual irá hospedar nosso site junto ao github, pressuponho que já tenha uma conta por lá, mas caso não tenha ainda é bem tranquilo de cadastrar uma, e para projetos públicos é totalmente gratuita. Já logado em sua conta você ira criar um novo repositório no canto superior direito, como pode ser visto nas imagens abaixo: Escolha um prefixo válido e único dentro do domínio github.io, para esse exemplo vou usar meu domínio pessoal que por completa falta de tempo está abandonado mesmo 😄. Após esse processo irá visualizar uma tela similar a essa: Agora precisamos iniciar nosso repositório para habilitá-lo no github pages, para isso vamos seguir os comandos para inicializar nosso repositório. 1234567echo \"# My personal site\" >> README.mdgit initgit add README.mdgit commit -m \"First Commit\"git remote add origin git@github.com:meneguite/meneguite.github.io.gitgit push -u origin master Após esse processo, é esperado que ao acessar seu repositório novamente visualize algo como: Agora já estamos prontos, se acessar seu endereço, no meu caso https://meneguite.github.io, já deve encontrar seu site já com https rodando normalmente. Assim é hora de avançar :). Preparando o site ao qual iremos publicarO segundo passo em nossa jornada será obter um site estático para usarmos de exemplo, durante esse artigo usarei o Hielo (https://templated.co/hielo), um template gratuito que utilizarei apenas para demonstrar a publicação durante esse artigo. Pelo link https://templated.co/hielo/download baixei um arquivo zip que descompactei na pasta do projeto e removi o arquivo inicial README.md que criamos no passo anterior para testes. A estrutura dos arquivos ficou assim: Agora vamos atualizar nosso repositório e publicar nosso novo site, para isso executaremos os seguintes comandos: 123git add .git commit -m \"Include new site implementation\"git push -u origin master Com esses comandos acima você já tem um deploy automatizado, bastando atualizar seu repositório e seu site já estará atualizado. Prontinho, nosso site já está no ar e completamente funcional, já é um endereço válido e se não possuir um domínio próprio já pode usar dessa forma sem maiores problemas. Configurando o DNS do seu domínio na CloudflareAgora precisamos de uma conta na CloudFlare, é um cadastro bastante simples, não terá nenhum problema em seguir o fluxo de cadastro deles, assim que terminar o processo irá entrar no dashboard onde poderá cadastrar seus domínios como imagem abaixo: Ao clicar em “+ Add Site” seguirá o seguinte fluxo: Selecione o plano mais indicado para a sua necessidade: E confirmar o plano selecionado novamente : Se já usa seu domínio em outro provedor de DNS nesse momento a CloudFlare irá buscar todos os registros atuais para que possa administra-los dentro de sua conta, no meu caso eu removi todos os registros do DNS o que deixou da seguinte forma: Agora precisamos adicionar dois registros, seu domínio principal e um para o subdomínio www, assim independente da forma que vier o acesso o mesmo será redirecionado corretamente. Eles devem ficar assim: A CloudFlare tem uma feature bem bacana no seu DNS chamada de CNAME Flattening que possibilita criar um CNAME para o root de um domínio. (Pode ler mais informações sobre isso em https://support.cloudflare.com/hc/en-us/articles/200169056-CNAME-Flattening-RFC-compliant-support-for-CNAME-at-the-root) Após configurado será lhe apresentado a seguinte tela: Nessa tela recebemos a solicitação de alterar o endereço de DNS junto ao nosso provedor do domínio. Esse passo varia de acordo com cada provedor, mas normalmente é bastante simples.. basta alterar o endereço dos servidores de DNS atuais para esses novos endereços. ** Precisa se atentar que está transferindo o DNS autoritativo para o serviço do CloudFlare e precisa ajustar registros como o MX para que não tenha nenhum problema na migração de seus e-mails por exemplo. Após alterar os endereços de DNS junto ao seu provedor, a propagação do mesmo pode demorar um tempo para acontecer, no caso do registro.br é indicado que pode demorar até 2 horas para essa transição. Ajustes finais para nosso domínioAinda dentro da cloudflare vamos acessar nosso domínio para finalizar alguns configurações adicionais bem importantes. A primeira aba que vamos alterar é a de Crypto. Dentro dessa aba devemos nos certificar que está configurado para Full, esse já é o padrão e provavelmente não tera que alterar nada. Após esse processo devemos habilitar o redirecionamento de HTTP para HTTPS por padrão, assim qualquer acesso vindo ao nosso site será redirecionado. Otimizações na entrega de conteúdoApós as configurações básicas passadas acima existem outras que podemos fazer para otimizar nosso site, esses passos são opcionais, mas podem ajudar bastante a aumentar a performance de sua aplicação. Na aba Speed, vamos encontrar algumas configurações para otimizar a entrega de conteúdo, a principal delas é a minificação dos assets de sua aplicação de forma automática, o que é extremamente útil. Existem diversas outras opções muito interessantes como o Rate Limiting que fica na aba de Firewall, que pode ser configurado também mas que não são tão importantes nesse momento, mas não deixe de dar uma olhada com mais calma em todas as opções que a CloudFlare disponibiliza. Após concluída a migração do DNS é esperado que seja mostrado essa mensagem na tela inicial do seu domínio dentro CloudFlare. Migrando sua Github Page para seu domínio definitivoApós finalizada a configuração necessária dentro da CloudFlare chegou a hora de transferir sua Github Page para o seu domínio. O processo é bastante simples, dentro do seu repositório no Github acesse a aba Settings, alterando em Custom domain para seu domínio como abaixo: Tudo correndo bem no processo, já temos nosso site no ar com o certificado SSL implementado, e com toda infraestrutura necessária para um site de pequeno e até de médio porte. Por hoje é isso, espero que esse artigo lhe seja útil. Qualquer dúvida ou sugestões de melhora deixe aqui nos comentários.","link":"/2018/11/10/publicar-um-site-com-github-pages-e-cloudflare/"},{"title":"Observer Pattern - Design Patterns com Typescript","text":"O Observer pattern é um dos padrões de projeto mais úteis, amplamente utilizado, especialmente no frontend com os novos frameworks/libraries reativas como Vue, React e Angular por exemplo, assim, entender bem seus conceitos e o problema que ele pretende resolver, é fundamental para que possamos usá-lo da melhor forma possível. O que é o observer pattern afinal?O Observer pattern é um padrão comportamental, assim como State, Iterator e Strategy (outros padrões de projetos que ainda vamos ver durante essa série de artigos), e como a maioria dos padrões desta categoria, trata da comunicação entre objetos. Por definição, o padrão Observer define uma dependência um-para-muitos entre objetos, que ao mudar o estado de um objeto (Subject), todos os observadores (Observers) são notificados e atualizados automaticamente, sendo que de uma forma mais simplista, a ideia é fazer com que uma mudança em um objeto do interesse de muitos (Subject) ao ser modificado, notifique a todos os interessados (Observers) sobre essa mudança, transferindo seu novo estado. Abaixo um diagrama que representa esse fluxo: Como tenho tentado nessa série de artigos, vou fazer o possível para trazer neste um exemplo mais concreto, mais fácil de perceber o problema que o padrão resolve, e como podemos implementar ele, de uma forma bem simplificada, mas que já vai lhe dar uma boa base de seu funcionamento. Para exemplificar, vamos usar um exemplo de um sistema de controle de temperatura, onde temos um sensor que lê a temperatura, e duas classes Fan (Ventilador) e TemperatureDisplay que dependem dessa informação do sensor para executar alguma ação, assim vamos implementar primeiro uma solução mais simplista para identificar o problema, depois implementamos o padrão Observer, bora ao código 😉. Primeiro precisamos definir o sensor: 12345678910111213141516171819202122232425interface TemperatureSensor { getTemperature(): Number;} class ArduinoTemperatureSensor implements TemperatureSensor { protected temperature: Number = 0; constructor() { setInterval(this.setNewTemperature.bind(this), 2000); } public getTemperature(): Number { return this.temperature; } protected setNewTemperature() { const randomTemperature = Math.floor(Math.random() * 120); console.info(`New Temperature: ${randomTemperature}`); this.setTemperature(Math.floor(randomTemperature)); } protected setTemperature(temperature: Number) { this.temperature = temperature; }} Uma implementação bem simples apenas para ilustrar, sendo que definimos um método “setNewTemperature” chamado a cada 2 segundos e que gera uma temperatura randômica para o sensor. Agora precisamos criar as classes que vão usar esse sensor: 12345678910111213141516171819202122232425262728293031323334353637class Fan { protected temperatureSensor: TemperatureSensor; protected running: boolean = false; constructor(temperatureSensor: TemperatureSensor) { this.temperatureSensor = temperatureSensor; setInterval(this.monitorTemperature.bind(this), 100); } public update(temperature: Number) { console.info(`Fan read temperature ${temperature}`); if (temperature < 50) { return this.turnOff(); } return this.turnOn(); } protected monitorTemperature() { const temperature = this.temperatureSensor.getTemperature(); this.update(temperature); } protected turnOn () { if (!this.running) { this.running = true; console.info('Fun started'); } } protected turnOff () { if (this.running) { this.running = false; console.info('Fun stoped'); } }} A classe Fan simula um ventilador, recebendo um sensor, e sempre que a temperatura atingir um valor maior que 50º, o mesmo é acionado, e desligado sempre que a temperatura for menor que esse valor. 1234567891011121314151617class TemperatureDisplay { protected readonly temperatureSensor: TemperatureSensor; constructor(temperatureSensor: TemperatureSensor) { this.temperatureSensor = temperatureSensor; setInterval(this.monitorTemperature.bind(this), 100); } public update (temperature: Number) { console.info(`Display: ${temperature}`); } protected monitorTemperature() { const temperature = this.temperatureSensor.getTemperature(); this.update(temperature); }} A classe TemperatureDisplay é ainda mais simples, apenas imprime no console a temperatura aferida pelo sensor. Assim poderíamos usar essas classes da seguinte forma: 123const arduinoTemperatureSensor = new ArduinoTemperatureSensor();const fan = new Fan(arduinoTemperatureSensor);const temperatureDisplay = new TemperatureDisplay(arduinoTemperatureSensor); Ao rodar esse código, alguns aspectos ficam muito claros: Uma classe que utiliza o sensor, tem sua precisão diretamente dependente da frequência que consulta o sensor; Fica claro também que se um número muito grande de objetos dependerem do sensor, teremos uma sobrecarga no sensor, talvez até atrapalhando função principal que seria capturar a temperatura; Para cada nova classe que precisa dos dados do sensor vamos precisar implementar uma nova lógica para garantir a precisão necessária para sua atualização; Assim, o padrão Observer, entra em cena exatamente para resolver esses e outros problemas introduzidos pela abordagem anterior. Implementando o Observer PatternPara implementação do padrão precisamos definir duas interfaces, Observer e Subject. 123456789interface Observer { notify(temperature: Number): void;}interface Subject { registerObserver(observer: Observer): void; unregisterObserver(observer: Observer): void; notifyObservers(): void;} Vamos agora estender nossa interface de TemperatureSensor para que possa se comportar como um Subject: 123interface TemperatureSensor extends Subject { getTemperature(): Number;} E alterar nosso sensor para essa nova interface: 1234567891011121314151617181920212223242526272829303132333435class ArduinoTemperatureSensor implements TemperatureSensor { protected temperature: Number = 0; protected observers: Observer[] = []; constructor() { setInterval(this.setNewTemperature.bind(this), 2000); } public registerObserver(observer: Observer): void { this.observers.push(observer); } public unregisterObserver(observer: Observer): void { this.observers = this.observers.filter(o => o !== observer ) } public notifyObservers(): void { this.observers.forEach((observer) => observer.notify(this.getTemperature())); } public getTemperature(): Number { return this.temperature; } protected setNewTemperature() { const randomTemperature = Math.floor(Math.random() * 120); console.info(`New Temperature: ${randomTemperature}`); this.setTemperature(Math.floor(randomTemperature)); } protected setTemperature(temperature: Number) { this.temperature = temperature; this.notifyObservers(); }} Reparem agora como a implementação das classes que dependem do sensor fica muito mais simplificada e precisa: 1234567891011121314151617181920212223242526272829303132class Fan implements Observer { protected temperatureSubject: Subject; protected running: boolean = false; constructor(temperatureSubject: Subject) { this.temperatureSubject = temperatureSubject; this.temperatureSubject.registerObserver(this); } public notify(temperature: Number) { console.info(`Fan read temperature ${temperature}`); if (temperature < 50) { return this.turnOff(); } return this.turnOn(); } protected turnOn () { if (!this.running) { this.running = true; console.info('Fan started'); } } protected turnOff () { if (this.running) { this.running = false; console.info('Fan stoped'); } }} 123456789101112class TemperatureDisplay implements Observer { protected readonly temperatureSubject: Subject; constructor(temperatureSubject: Subject) { this.temperatureSubject = temperatureSubject; this.temperatureSubject.registerObserver(this); } public notify(temperature: Number) { console.info(`Display: ${temperature}`); }} Podemos assim executar como antes: 123const arduinoTemperatureSensor = new ArduinoTemperatureSensor();const fan = new Fan(arduinoTemperatureSensor);const temperatureDisplay = new TemperatureDisplay(arduinoTemperatureSensor); Como puderam perceber com essa mudança de abordagem, ganhamos uma precisão muito superior, fazendo com que todos objetos que dependam dos dados do sensor sejam atualizado imediatamente após uma nova leitura do sensor, além disso temos algumas outras vantagens, como por exemplo a simplificação das classes que precisam dos dados do sensor e principalmente removemos o acoplamento entre os objetos, fazendo com que interajam normalmente, porém conhecendo muito pouco um do outro. O Código completo para esta implementação pode ser encontrado em: https://github.com/meneguite/typescript-design-patterns/blob/master/observer/index.ts Outros padrões de Projetos Adapter Pattern Observer Pattern","link":"/2019/06/23/design-patterns-com-typescript-observer/"},{"title":"Como remover arquivos antigos do slack","text":"O Slack é uma ferramenta fantástica, se fazendo cada vez mais presente no dia a dia de empresas de todos os portes, seu plano gratuito é muito bom, já atentendo atender a demanda da grande maioria de empresas de pequeno porte, porém ele tem uma limitação bem chata, ao atingir o limite de armazenamento, hoje quando escrevo esse artigo 5GB, você precisa remover arquivo a arquivo em um processo extremamente manual e desestimulante. Nesse artigo pretendo mostrar um utilitário open source que desenvolvi e que irá lhe permitir executar esse processo de forma automática apenas passando o período que deseja remover os arquivos. O processo de remoção manual do slack é irritante, e como desenvolvedor que sou, detestaria fazer um processo manual como esse, ainda mais se tenho outra saída melhor para isso, assim resolvi escrever um utilitário, e posteriormente esse artigo, para demonstrar como podemos fazer esse processo em poucos passos, o que poderá te poupar várias horas limpando arquivos de seu workspace no slack. Para iniciarmos vamos precisar criar um token legacy no endereço https://api.slack.com/custom-integrations/legacy-tokens, bastando para isso gerar um novo token, caso já não o tenha gerado um até então. É esse token que irá lhe permitir executar essas ações em seu workspace. Para executar o utilitário vai precisar ter instalado em sua maquina uma versão recente do node, se já não possui ele por ai, a instalação é bem simples e encontrara todos os passos necessários em https://nodejs.org/, por aqui estou usando a versão 10, a LTS no momento que escrevo o artigo, mas provavelmente não vai ter problemas para rodar esse utilitário em versões mais recentes. Com o node instalado vamos instalar nosso utilitário: 1npm install slack-remove-files -g Após o processo é só executar o comando: 1slack-remove-files 12 months {SEU_TOKEN_AQUI} Nesse exemplo, irá remover todos os arquivos anteriores a 12 meses, mas não se limite ao exemplo, variando o valor e unidade pode personalizar como quiser e lhe atender melhor, alguns outros exemplos de combinação: 12slack-remove-files 60 days {SEU_TOKEN_AQUI}slack-remove-files 2 years {SEU_TOKEN_AQUI} Fique a vontade para criar sua combinação, antes de executar ele irá lhe solicitar uma confirmação mostrando a data apurada como pode ser visto abaixo: 12All files dated less than will 2017-08-03T16:57:04.626Z be removed. Do you wish to continue ? (y ou n) Apenas se confirmar é esperado que ele busque junto ao slack todos os arquivos e remova um a um, apresentando um relatório dos arquivos removidos como mostrado na imagem abaixo: O Código completo para desse utilitário pode ser encontrado em: https://github.com/meneguite/slack-remove-files Espero que lhe seja útil! Caso tenha alguma dúvida sobre o uso, pode deixar por aqui, ou o que seria ainda melhor, usando as issues do github.","link":"/2019/08/02/como-remover-arquivos-antigos-do-slack/"},{"title":"PWA - More than Web Apps, Progressive Web Apps!","text":"Falar sobre PWA é algo que sempre curto bastante, e ontem tive a oportunidade de apresentar uma talk em nosso grupo do GDG (Google Developer Groups) Cataguases, onde tentei passar minha visão sobre esse tema e demonstrar o quão relevante será seu futuro ( o presente também é claro :) ), passando os conceitos envolvidos no tema, bem como ferramentas e soluções que o completam e fazem das progressive web apps, ao meu ver, o futuro da esmagadora maioria das aplicações. Abaixo deixo os slides da talk, feedback são sempre bem vindos ;)","link":"/2019/09/09/pwa-processive-web-apps/"},{"title":"Quando não usar o MongoDb","text":"MongoDb é uma solução incrível, e tenho visto ganhar mercado de forma surpreendente, as facilidades e simplificação no fluxo de desenvolvimento acaba arrebatando o coração de muitos desenvolvedores, porém o que me preocupa, e o motivo que me fez escrever esse artigo, é que talvez está ganhando um mercado e sendo usado em lugares que muito provavelmente ele não é adequado. Sempre ao escolher uma tecnologia é extremamente importante se levar em consideração as limitações e propósitos da mesma, isso vai com certeza evitar uma enorme dor de cabeça no futuro. Antes de seguirmos gostaria de deixar bem claro que não tenho nada contra o MongoDb, ao contrário, uso extensivamente ele em muitos projetos nos quais ele é aderente, mas ele não é uma bala de prata para resolver todos os problemas, saber onde e quando usá-lo é muito importante. Outro ponto que gostaria de deixar claro é que muitos itens que irei levantar aqui cabem a outros bancos NoSql, direciono ao MongoDb apenas por ser a tecnologia que percebo estar mais associada a esses “errors” na escolha de storage de dados, muitas vezes sendo usado apenas como um banco relacional. Eric Evans no livro Domain-Driven Design de 2015 escreveu “The problem is not the relational databases, but when you use then for everything” em uma tradução livre, o problema não são os bancos de dados relacionais, mas quando você os usa para tudo. Hoje 5 anos após a publicação poderíamos falar exatamente o mesmo para bancos não relacionais e em especial para o MongoDb. Bancos NoSql como o mongoDb, são sistemas de armazenamento muito bons em resolver problemas como a escalabilidade e alta disponibilidade, em especial a escalabilidade horizontal, normalmente um grande gargalo de aplicações que usam bancos relacionais e precisam ter baixo tempo de resposta, mas são mais eficientes em armazenar objetos que não precisam ser normalizados. Quando não se deve usar o mongoDb?Quando a equipe não domina o MongoDbMongoDb não é daquelas ferramentas que da pra descobrir como funciona com o projeto andando, é necessário ao menos alguém com o domínio dos fluxos, configurações e da própria ferramenta. Caso tenha uma equipe com proficiência apenas em bases de dados relacionais e não tenha o tempo necessário para capacita-la, mongoDb não é uma boa. Desenvolver uma aplicação, configurar e manter um cluster MongoDb não são tarefas triviais e terá um penoso caminho até ter uma equipe proficiente nessas tarefas. Muitos erros e problemas associados ao MongoDb são na verdade fruto de uso inadequado do mesmo. Quando precisa garantir a integridade transacional e ACIDQuando transações atômicas e isoladas forem a única forma de resolver o problema, mongoDb não é interessante. Ainda que versões mais recentes possuam suporte a transações inclusive multi documentos como pode ser lido na documentação (https://docs.mongodb.com/manual/core/transactions/) quando sua aplicação depende disso para seu bom funcionamento é melhor evitar o uso do mongo. Na própria documentação do mongo é possível encontrar o seguinte alerta: Outras características interessantes e que precisam ser levados em consideração são a “Eventual Consistency” ou consistência eventual, onde os dados ficarão consistente em algum momento, porém não garantem a consistência o tempo todo e o “Soft-state” onde os dados não precisam ser consistentes com gravação, nem réplicas precisam ser mutualmente consistentes o tempo todo. Essas características são pontos de atenção que podem gerar problemas muito graves se não tratadas adequadamente, como por exemplo receber uma resposta não atualizada quando se busca o saldo de uma conta bancária. Quando possui uma demanda muito específicaA proposta do MongoDb é ser um banco de uso geral, ele possui quase todas as funcionalidades e características necessárias para uso de um banco de dados completo, isso certamente é um dos fatores importantes que possibilitam usar o mongo como fonte de dados principal em uma aplicação, ao contrário de outras soluções NoSql que possuem um escopo muito mais especializado. Considerando que o mongo precisa atender a grande maioria dos cenários do dia a dia é evidente que outras soluções mais especializadas serão mais adequadas e atenderão melhor nesses cenários, assim é comum complementar o uso do mongo com outras soluções que para que se propõe, são melhores. Por exemplo: Quando possui dados que sempre são acessados apenas por uma chave, bancos chave-valor como Redis e Memcache são bem interessantes. Esses sistemas são constantemente usados para armazenamento de cache e configurações por exemplo; Quando se precisa de um sistema mais parrudo de busca talvez soluções como Elasticsearch ou Solr podem ser mais adequadas; Quando se precisa de um sistema mais confiável de filas (queues), provavelmente usar soluções mais especializadas como RabbitMQ e ZeroMq por exemplo, é melhor. Quando os dados forem claramente relacionaisMongoDb é um banco não relacional, tenha isso sempre em mente. Ainda que possua suporte a relacionamentos, não é onde ele se dará melhor. Se tem um domínio bem definido e claramente uma estrutura relacional, é melhor confiar seus dados a um gestor de banco de dados que foi criado e otimizado para manter os relacionamentos e consistência entre os mesmos. A garantia de integridade relacional no MongoDb não existe, passa a ser responsabilidade da aplicação garantir essa integridade, o que pode gerar muitos problemas de inconsciência se sua equipe não possuir maturidade para assumir essa responsabilidade. Por exemplo remover um usuário sem remover os endereços desse usuário, vai gerar uma série de registros órfãos no banco. Quando o custo de manter um cluster for um problemaManter um cluster mongo não é barato, idealmente é sugerido manter ao menos 3 instancias do cluster com máquinas não muito fracas, normalmente acima de 2GB de RAM para garantir a integridade do mesmo. É possível rodar com 2 instancias mais um arbitro, mas é um risco que normalmente não aconselho quando não se sabe exatamente o que se está fazendo. É claro que esse custo pode ser irrisório em projetos maiores e que já estejam tracionando, porém podem pesar quando se pretende apenas validar um produto, ou ainda possui um produto que não se paga. Então tenha em mente o custo total que manter um cluster mongo vai trazer para o projeto. ConclusãoO MongoDB é um banco de dados incrível e com certeza não deve ser descartado como um possível storage de dados em seus projetos, apenas certifique-se que o use onde é adequado, assim conseguira aproveitar o que de melhor o mesmo tem a oferecer e evitara problemas e dores de cabeça por uma escolha equivocada.","link":"/2021/01/01/quando-nao-usar-o-mongodb/"},{"title":"LDAP - Lightweight Directory Access Protocol","text":"Este documento tem por objetivo orientar e discorrer sobre um protocolo que, apesar de ser muito utilizado, sendo um padrão em empresas de médio a grande porte, são poucos os que realmente conhecem suas funcionalidades mais avançadas. Isso ocorre visto que soluções proprietárias tendem a abstrair esse nível de configuração, engessando um protocolo extremamente flexível, limitando-os a apenas frações de suas funcionalidades. No decorrer desse material o leitor será encaminhado por uma introdução sobre os conceitos que são necessários para o entendimento do protocolo, passando pelas implementações dessa ferramenta, hoje disponíveis no mercado; um exemplo de estudo de caso, onde a partir de uma necessidade inicial de um cliente é desenvolvido um estudo de quais ferramentas mais se adequariam aos requisitos, sempre focando a centralização das informações em uma única base LDAP. O PROTOCOLO LDAPOrigens do LDAPNo início da década de 80, ao se unirem a ISO e o CCITT com o objetivo de criar um serviço de mensagens, surgiu a necessidade de desenvolver um protocolo que tivesse a capacidade de organizar entradas em um serviço de nomes de forma hierárquica, capaz de suportar grandes quantidades de dados e com uma enorme capacidade de procura de informações. Esse serviço criado pelas duas instituições, foi apresentado em 1988, sendo denominado X.500, juntamente com um conjunto de recomendações e normas ISO 9594. O X.500 especificava que a comunicação entre o cliente e o servidor do diretório deveria usar o Directory Access Protocol (DAP) que era executado sobre a pilha de protocolos do modelo OSI. Devido à alta complexidade e o custo elevado, pesquisadores da Universidade de Michigan criaram um servidor LDAP, o slapd, que atuava sobre os protocolos TCP/IP. Este servidor foi apresentado como uma alternativa ao protocolo DAP em 1993, como citado por GOUVEIA (2005), disponibilizando as fontes na Internet e criando listas de discussão para divulgar e aperfeiçoar esse novo protocolo. Assim a evolução do mesmo foi acompanhada por pessoas do mundo inteiro, e o mesmo deixou de ser uma mera alternativa para o protocolo DAP, tornando-se um serviço de diretório completo, agora competindo diretamente com o X.500. O LDAP é um protocolo especializado em organizar os recursos de rede de forma hierárquica, através de uma árvore de diretórios, que roda sobre os protocolos TCP/IP, diferente do protocolo no qual foi baseado, o DAP, o qual roda sobre o modelo OSI. Para BARTH e SIEWERT (2009) essa foi uma das principais causas da adoção em larga escala do protocolo, visto que com essa nova plataforma foi possível reduzir consideravelmente o overhead1 de camadas superiores do modelo OSI. Segundo GOUVEIA (2005) o LDAP ganhou força após o ano de 1997, quando foi lançada sua terceira versão, além de uma fundação a qual mantém uma solução OpenSource a OpenLDAP Foudation, várias outras empresas como Novell, Microsoft e Netscape começaram a oferecer produtos baseados nessa nova plataforma. Protocolo LDAP vs X.500Segundo TRIGO (2007), o LDAP possui as seguintes simplificações em relação ao X.500: É executado diretamente sobre o TCP/IP; A maioria dos elementos de dados são representados como cadeias de caracteres, processadas de modo mais fácil que os dados na representação estruturada Abstract Syntax Notation One (ASN.1) usada pelo X.500; Codifica dados para transporte em redes usando uma versão simplificada das mesmas regras de codificação usadas pelo X.500; Elimina características pouco usadas e também operações redundantes do X.500. Figura 1: Aplicação LDAP Fonte: OpenLDAP Foundation (2009) Noções teóricas sobre o LDAPDefinições de diretóriosTRIGO (2007) descreve que diretório é literalmente definido como “algo usado para indicar direções”, ou seja, para indicar um caminho para se chegar àquilo que se procura. Segundo a definição de TUTTLE (2009): “Diretório é uma lista de informações sobre objetos organizados ou catalogados em uma ordem, e que fornece o acesso aos dados dos objetos. São os diretórios que permitem que usuários ou aplicações encontrem recursos no ambiente com as características necessárias para um tipo de tarefa particular.” Diretórios nos cercam a todo tempo, seja em uma lista telefônica, na estrutura de pastas do computador, em blogs, em serviços de buscas, além de vários outros lugares. Outro exemplo de diretório é o DNS o qual possui uma relação de nomes de host2 e seu respectivoIP. Segundo TRIGO (2007) é muito comum ocorrer confusão com o uso de diretórios,pois apesar de ser possível fazer com eles praticamente qualquer coisa, desde salvar informações como um banco de dados, salvar arquivos como um sistema de arquivos e disponibilizar arquivos como um sistema FTP, não se justifica o mesmo, pois para cada uma dessas funções existe um sistema que foi desenvolvido para fazer somente essa tarefa, assim sem dúvida, o fará muito melhor do que o diretório. Características do LDAPSegundo TRIGO (2007), o LDAP foi padronizado em junho de 1993, no RFC 1487 da IETF. O LDAP, segundo BARTH e SIEWERT (2009) foi projetado para resolver problemas de distribuição de diretórios pela rede, contando com nove aspectos que lhe garantiram essa habilidade, sendo eles: Seu desenho genérico Simplicidade do protocolo Arquitetura distribuída Segurança Padrão aberto Solicitação de funcionalidades e esquemas do servidor Internacionalização Suporte ao IPv6 Berkeley ́s Data Base (BDB) Ainda segundo BARTH e SIEWERT (2009) uma aplicação que o LDAP disponibiliza é o conceito de Single Sign On (SSO), ou seja, autenticação unificada, possibilitando e facilitando a integração com diversos outros serviços, assim o usuário tem apenas um userid ou identificação única na rede e com esse pode acessar diversos recursos da mesma. TUTTLE (2009) afirma que o LDAP é um padrão aberto capaz de facilitar, de forma flexível, o compartilhamento, a manutenção e o gerenciamento de grandes volumes de informações, definindo um método-padrão de acesso e atualização de informações dentro de um diretório.Para LOSANO (2009) a principal característica do LDAP é a integração com outros serviços, complementando assim a infra-estrutura de redes, fornecendo novos recursos e, especialmente, maior integração, diferentemente de outros protocolos e linguagens estabelecidos como exemplo: SNMP, HTTP, SMTP, IMAP ou SQL. Abaixo a figura 2 representa essa característica. Figura 2: Característica do LDAP Fonte: OpenLDAP Foundation (2009) Estrutura do LDAPTRIGO (2007) afirma que o grande fator responsável pelaflexibilidade do LDAP é a sua organização de forma hierárquica. A árvore de informações possui um elemento-raiz, por onde começa a busca das informações. A partir daí, o sistema vai percorrendo os nós filhos até que consiga encontrar o elemento desejado. A raiz e os ramos da árvore são os diretórios os quais podem conter outros diretórios. Abaixo desses diretórios estão os elementos ou também chamados de entradas. Para cada entrada podemos ter um ou mais valores associados a ela. A tabela 1 abaixo mostra os principais atributos e suas descrições. Tabela 1: Exemplos de atributos, com suas respectivas descrições: Atibuto Descrição Dc Identificação única do objeto (do inglês Distinguished Name) C Para diretórios que representam países (do inglês country) O Para o nome da empresa (do inglês organization) Ou Para departamento (do inglês organization unit) Cn Atributo de nome (do inglês common name) Uid Identidade do usuário (do inglês user id) Gn Nome próprio do usuário (do inglês given name) Segundo TRIGO (2007), existem duas maneiras de organizar uma árvore de diretórios; no estilo X.500 onde a estrutura da árvore de diretórios é baseado em regiões, como é demonstrado na figura 3 e no estilo DNS, no qual, os dados são organizados como se fossem domínios. A grande vantagem de se usar o estilo DNS é o fato de poder configurar o serviço de LDAP para uma empresa a partir de um nome de domínio válido, garantindo o caráter único da identidade quando disponibilizado através da Internet. Na figura 4 abaixo podemos visualizar uma estrutura baseada nesse estilo. Figura 3: Estrutura no estilo X.500 Figura 4: Estrutura no estilo DNS #### Schema Segundo GOUVEIA (2005), os schemas são responsáveis por manter a integridade dos dados do diretório. São extensíveis, possibilitando a adição de atributos ou classes de acordo com a necessidade. Schemas definem quais object class podem ser inseridos no diretório, quais os atributosde uma determinada object class e quais os valores possíveis para esses atributos. Assim, caso um objeto não obedeça às regras do schema, não poderá ser inserido no mesmo. Arquivos LDIFSegundo TRIGO (2007), arquivos LDIFs são arquivos de texto puro, usados para importar, modificar e exportar informações. Esse formato de arquivo é o único meio de entrada de dados em um servidor LDAP; mesmo programas que trabalham diretamente com o servidor LDAP, inserindo e removendo registros, como é o caso do PHPMyAdmin, utilizam-se de arquivos LDIF para suas transações. Abaixo é colocado dois arquivos LDIF onde o primeiro é responsável pela inserção de um usuário em uma base, e o seguinte pela modificação do valor do atributo userPassword, no caso, a senha do usuário. Arquivo insereUser.ldif 12345dn: cn=ronaldo,dc=secure,dc=inf,dc=brcn: ronaldoobjectClass: simpleSecurityObjectobjectClass: organizationalRoleuserPassword: {SSHA}4P4F2gCtCh7PthUlS1AJ+DOMXOI0iUpH description: Usuario Ronaldo Arquivo atualizaUser.ldif 1234dn: cn=ronaldo,dc=secure,dc=inf,dc=brchangetype: modifyreplace: userPassworduserPassword: {SSHA}tNnd/KF/Rg/dCTq4S1yo7OEiUK4aFk9g IMPLEMENTAÇÕES DO PROTOCOLO LDAPComo mencionado anteriormente, o protocolo LDAP é apenas um conjunto de conceitos e definições que possibilitam uma padronização na troca de informações entre diversas soluções baseadas no protocolo. Assim os métodos utilizados para armazenar as informações internamente e de replicação, por exemplo, são peculiaridades de cada implementação. Abaixo é apresentado algumas características dessas principais implementações. Active DirectoryO Active Directory é uma implementação do serviço de diretório utilizando o protocolo LDAP que armazena informações sobre objetos em redes de computadores e disponibiliza essas informações a usuários e administradores desta rede. É um software da Microsoft utilizado em ambientes Windows e que segundo SANTANA (2009) surgiu juntamente com o Windows 2000 Server. Objetos como usuários, grupos, membros dos grupos, senhas, contas de computadores, relações de confiança, informações sobre o domínio, unidades organizacionais, etc., ficam armazenados no banco de dados do AD que além de armazenar esses vários objetos em seu banco de dados, disponibiliza vários serviços, como: autenticação de usuários, replicação do seu banco de dados, pesquisa dos objetos disponíveis na rede, administração centralizada da segurança utilizando GPO, entre outros. Esses recursos tornam a administração do AD bem mais fácil, sendo possível administrar todos os recursos disponíveis na rede centralizadamente. Figura 5: Exemplo de implementação do Active Directory eDirectoryAssim como a Microsoft, a Novell também disponibiliza uma implementação do LDAP, o eDirectory, que é a base de identidade que vincula os usuários e seus direitos de acesso aos recursos, dispositivos e políticas de segurança da empresa. Ele oferece a compatibilidade, segurança, confiabilidade, escalabilidade e gerenciabilidade necessárias para distribuições internas e de Internet. O eDirectory possui características muito próximas do Active Directory da Microsoft. Figura 6: Exemplo de implementação do eDirectory Fonte: Novel (2009) OpenLDAPO OpenLDAP é uma implementação do LDAP desenvolvida pela Universidade de Michigan e mantido pelo Projeto OpenLDAP , possui como principais características: suporte ao IPv4 e IPv6, autenticação, segurança no transporte usando SSL e TSL, controle de acessos, alta performance em múltiplas chamadas e a replicação de base. O OpenLDAP tem uma licença específica chamada de The OpenLDAP Public License (OpenLDAP Project, 2009) e é independente de plataforma, assim várias distribuições Linux já disponibilizam o mesmo em seus repositórios. Além do Linux o OpenLDAP é também compatível com AIX, variantes de BSD, HP-UX, Mac OS X, Solaris e Microsoft Windows ( Baseados na tecnologia NT). Figura 7: Implementação do LDAP + phpLDAPAdmin ReplicaçãoA replicação é um método muito utilizado quando trabalhamos em um ambiente corporativo, onde existe a necessidade de alta disponibilidade nos serviços. É na verdade a manutenção de uma cópia, seja parcial ou total, dos dados em outros servidores. No OpenLDAP existem duas técnicas distintas de replicação, as quais são o slurpd e o syncrepl, sendo que a slurpd, é uma técnica mais antiga a qual foi descontinuada na versão 2.4 do OpenLDAP , por possuir uma série de limitações que foram supridas por sua sucessora a syncrepl. As principais limitações eram as limitações para replicar uma base parcialmente e a utilização de mais de um servidor Master. Quando trabalhamos com o syncrepl o servidor LDAP pode assumir duas posturas quanto à replicação, Master ou Slave. Quando Master o servidor assume todas as funções de um servidor OpenLDAP , desde a inserção, atualização e consulta dos dados. Porém quando slave o servidor só aceitará execuções de consultas em sua base de dados quando originadas de máquinas clientes, assim para que qualquer informação seja alterada, a solicitação deveráser feita ao servidor Master o qual se encarregará de replicar as alterações para seus servidores slaves. Replicação Master x Slave – Nesse método de replicação temos um único servidor Master o qual tem seu conteúdo replicado em todos os seus servidores escravos. Nesses servidores slaves não ocorre entrada de dados, apenas replicam passivamente os dados de um servidor principal. Esse método de replicação é o mais comum e atende a grande parte das demandas de servidores LDAP. Replicação Master x Master (multimaster) – Replicação multimaster é um método de replicação mais recente no OpenLDAP , porém já implementado no Active Directory desde sua primeira versão. Esta é uma replicação que atende a demandas muito singulares e apesar de funcionar muito bem ainda é pouco difundida e aplicada.Esta replicação consiste na utilização de dois ou mais servidores Masters, independentes, os quais possuem todas as suas funcionalidades ativadas, porém é criado por parte dos servidores um controle interno para que seja possível manter a integridade dos dados, algo que era muito mais simples de controlar quando existia uma única entrada de dados. Diretórios Distribuídos – Utilizando o OpenLpad com o método de replicação syncrepl podemos não somente criar uma cópia da árvore de diretórios em outros servidores, mas também criar políticas de replicação e replicar a cada servidor somente aquilo que se pretende que o mesmo tenha acesso, assim por exemplo, quando temos uma empresa com sede em Minas Gerais e filial em São Paulo, a mesma não precisa replicar toda sua base para a filial, apenas aqueles usuários que a pertençam, evitando assim um tráfego desnecessário narede além de aumentar a segurança. CriptografiaFazendo-se uso da definição de FADEL (2009): “O termo Criptografia tem origem grega e surgiu da fusão das palavras “kryptós” e “graphein”, que significam “oculto” e “escrever”, respectivamente. Trata-se de um conjunto de conceitos e técnicas que visa codificar uma informação de forma que somente o emissor e o receptor possam acessá-la, evitando que um intruso consiga interceptá-la.” Segundo TRIGO (2007), na maioria das vezes, o servidor LDAP é utilizado para armazenar dados de usuários, como senha de autenticação, por exemplo, assim segurança é algo fundamental. Para aumentarmos a segurança no transporte de dados é possível criptografá-los, usando TLS ou SSL, assim mesmo que alguém consiga interceptar os dados, não conseguirá visualizar o que está sendo trafegado. Módulos de Banco de DadosComo mencionado anteriormente o LDAP é apenas um protocolo de comunicação entre um cliente e um serviço de diretórios. A definição do armazenamento das informações da árvore de diretórios é independente do mesmo, assim cada implementação do protocolo é responsável por fazer essa definição, podendo variar desde um simples arquivo texto até um banco de dados relacional completo. Segundo TRIGO (2007), o OpenLDAP possui dois bancos de dados nativos, o LDBM e o BerkeleyDB, sendo que o segundo é para ele a melhor opção quanto ao desempenho. Listas de Controle de Acesso (ACLs)ACL é a definição de todos os recursos de acesso controlado e todos aqueles usuários que têm acesso a eles. Segundo CARTER (2009), as ACLs disponibilizadas pelo OpenLDAP possuem uma sintaxe simples, além de serem também muito flexíveis e robustas em sua implementação. A idéia básica das ACLs é definir quem tem acesso a quê. Abaixo é exposto um exemplo de ACL no OpenLDAP : 12345# ACL do Atributo userPassword Access to attrs=userPassword by self write by cn=backup,dc=secure,dc=inf,dc=br read by * auth A ACL acima tem a função de permitir que apenas o dono tenha acesso de escrita no campo userPassword, ou seja, só ele pode mudar sua senha, que o usuário cn=backup,dc=secure,dc=inf,dc=br consiga apenas ler esse atributo e que todos os demais deverão se autenticar como um usuário com permissão de acesso a esse atributo. Resumidamente a sintaxe das ACLs no OpenLDAP é: 1access to <o que> by <quem> <controle> Backups e RestauraçãoO backup e restauração de uma base LDAP é extremamente fácil, como demonstrado com os comandos abaixo: Para efetuarmos Backup de toda a base: 1slapcat > backup.ldif Retornar o backup (é necessário estarmos com a base parada) 1slapadd –l backup.ldif Existem outras maneiras de fazer os procedimentos acima relacionados. Por exemplo, usando o ldapadd, não precisamos parar o serviço do OpenLDAP para efetuarmos o retorno dos dados, porém precisaremos ter um usuário com permissão administrativa para inserir os mesmos, diferente do slapcat o qual só precisa ter acesso ao diretório do banco de dados do OpenLDAP no caminho /var/lib/ldap/. Saber definir qual o procedimento mais adequado para a necessidade do cliente é extremamente importante. ESTUDO DE CASOLevantamento de requisitosPara uma demonstração prática sobre o tema foi feito um estudo de caso da empresa Secure Info Ltda, uma empresa de consultoria na área de segurança da informação a qual está se preparando para entrar no mercado. A Secure Info possui demanda de uma série de serviços os quais pretende prover internamente, sendo eles: Servidor interno de e-mail (com soluções SMTP, POP e IMAP); Sistema gerenciador de domínio e autenticação (máquinas Windows e Linux); Servidor FTP; Sistema de Webmail; Servidor de Proxy; Servidor Web; Servidor SSH; Servidor de Arquivos. Para esta implementação foi definido como requisito a integração da autenticação entre os serviços, assim não deverá haver redundância de informações cadastrais dos usuários e lista de contatos, outro requisito é a utilização de soluções OpenSource de maneira a não dispor de recursos na aquisição de novas licenças. Este documento abordará somente a instalação e configuração dos servidores LDAP, informações sobre a instalação dos outros serviços citados podem ser encontradas no próprio website do desenvolvedor, onde normalmente já disponibilizam materiais com instruções para a instalação, como também nos livros de CARTER (2009) e TRIGO (2007). Definição de Softwares a serem usadosSistema operacionalPara a implementação destes servidores o primeiro item a ser definido é o sistema operacional que será usado, pois todos os softwares definidos posteriormente dependerão diretamente dessa definição. Diversos sistemas foram considerados nessa implementação, porém alguns como o Windows Server e Unix foram descartados por termos como requisito, um sistema OpenSource o qual não demande custo de licenças, o que não é o caso dos mesmos. Assim se enquadram nesse perfil os sistemas LINUX, porém os mesmos possuem uma infinidade de derivações, denominadas distro ou distribuições, as quais se diferenciam em diversos casos muito uma da outra, assim escolher uma distribuição que se enquadre melhor aos requisitos do projeto é fundamental para o sucesso do mesmo. As distribuições avaliadas foram: 123456Suse Linux Enterprise 11;OpenSuse 11.1;RedHat Enterprise 5.4;Fedora 11;Unbutu Server 8.04 LTS;Debian 5; Diante das distribuições avaliadas, duas inicialmente já foram descartadas por se tratarem de distribuições comerciais as quais demandam um custo elevado para aquisição da licença de uso, são elas: Suse Linux Enterprise 11 e RedHat Enterprise 5.4. Das demais, todas cumpririam os pré-requisitos identificados, assim definimos o Debian 5 como a solução a ser implementada, pois trata-se de um sistema robusto, leve, possui um gerenciador de pacotes extremamente poderoso, o APT, além de contar com um sistema massivo de testes que garantem que os softwares disponibilizados para o mesmo, estarão realmente maduros, sendo esse seu principal diferencial. Sistema Gerenciador de Domínio e AutenticaçãoPara essa definição foram considerados as três principais implementações do protocolo LDAP, o Active Directory, eDirectory e uma solução integrada entre OpenLDAP + Samba, porém a única solução que cumpre os requisitos é a junção OpenLDAP + Samba, já que os outros são proprietários e demandam um investimento alto na aquisição de suas licenças. Sistema de e-mailDiversos sistemas gerenciadores de e-mails também conhecidos como MTA estão disponíveis hoje no mercado, os principais são Microsoft Exchange, Qmail e Postfix. O Microsoft Exchange é um sistema muito bom, porém não é compatível com o sistema operacional adotado, assim restam-nos duas soluções o Qmail e PostFix, diante das mesmas optaremos pela solução Postfix a qual conta com um desenvolvimento bem mais ativo que o Qmail, além de possuir material de apoio muito bom e amplo. Servidor FTPExiste hoje uma infinidade de opções em soluções FTP, como por exemplo, proftpd, pureftp e o vsftpd, porém as mesmas possuem características muito similares sendo assim optaremos pela implementação da solução proftpd a qual atende os requisitos e possui uma maior compatibilidade de integração com o servidor de autenticação baseada no OpenLDAP. Servidor de ArquivoPara servidor de arquivo utilizaremos o SAMBA por atender os requisitos iniciais do projeto além de já estar disponível no sistema devido a necessidade do mesmo para autenticarmos os usuários na rede. Definição da divisão dos serviços entre os servidoresPara esta implementação usaremos a seguinte estrutura: Figura 8: Estrutura de divisão dos serviços entre os servidores Cada figura de servidor representa um serviço rodando dentro do respectivo servidor. Instalação do Servidor OpenLDAPRequisitos para a instalação do OpenLDAPO primeiro procedimento adotado foi a atualização dos repositórios instalados com o comando: 1apt-get update Logo depois a atualização dos pacotes instalados: 1apt-get upgrade Antes de iniciar a instalação verificamos alguns parâmetros importantes no sistema, os quais, farão grande diferença no ato da instalação do OpenLDAP . O primeiro parâmetro verificado se encontra no arquivo /etc/hosts, onde deverá ser informado o IP do servidor, nome, domínio e apelido da máquina. Foram inseridos os seguintes parâmetros logo abaixo da linha, que referencia o localhost. 12127.0.0.1 serv1.secure.inf.br serv1 192.168.239.134 serv1.secure.inf.br serv1 O segundo parâmetro verificado foi no arquivo /etc/hostname onde substituímos o nome serv1 para seu nome completo serv1.secure.inf.br. Com esses parâmetros configurados cumprimos os requisitos básicos para iniciarmos a instalação. Instalação do OpenLDAP e seus utilitáriosPara a instalação do OpenLDAP e seus utilitários utilizamos o seguinte comando: 1apt-get install slapd ldap-utils Na instalação inicial, o próprio debian cria baseado nos parâmetros os quais foram alterados anteriormente uma estrutura previa da árvore, assim não precisamos fazer esse processo manualmente. Nesta instalação a árvore em formato DNS foi criada com a raiz “dc=secure,dc=inf,dc=br”, parâmetro esse que servirá de base para todos os itens que o sucederem. O arquivo de configuração principal do OpenLDAP é o /etc/ldap/slapd.conf, este éalgo como um centro de controle do OpenLDAP e é nele que faremos várias configurações durante todo o processo de instalação do servidor. Com a base montada procedemos com o seguinte comando para verificar se o servidor OpenLDAP está de fato configurado corretamente: 1ldapsearch -x -h localhost -b \"dc=secure,dc=inf,dc=br\" O retorno foi: 123456789101112dn: dc=secure,dc=inf,dc=brobjectClass: top objectClass: dcObjectobjectClass: organization o: secure.inf.brdc: doctumdn: cn=admin,dc=secure,dc=inf,dc=brobjectClass: simpleSecurityObject objectClass: organizationalRolecn: admindescription: LDAP administrator Com esse retorno demos sequência à instalação inserindo alguns objetos na árvore, de maneira a povoá-la adequadamente. Como descrito anteriormente, a única maneira de transferir dados a um servidor LDAP é fazendo uso de arquivos do tipo ldif, assim prosseguiremos criando um arquivo, o qual será o responsável por criar alguns grupos em nossa árvore. Abaixo podemos visualizar o conteúdo do arquivo grupos.ldif : 123456789101112131415161718192021222324dn: ou=grupos,dc=secure,dc=inf,dc=br ou: GruposobjectClass: organizationalUnit objectClass: topdn: ou=usuarios,dc=secure,dc=inf,dc=br ou: GruposobjectClass: organizationalUnit objectClass: topdn: ou=ti,ou=usuarios,dc=secure,dc=inf,dc=br ou: tiobjectClass: organizationalUnitobjectClass: topdn: ou=base_teste,ou=usuarios,dc=secure,dc=inf,dc=br ou: base_testeobjectClass: organizationalUnitobjectClass: topdn: ou=administracao,ou=usuarios,dc=secure,dc=inf,dc=br ou: administracaoobjectClass: organizationalUnitobjectClass: top Para inserirmos esses grupos em nossa árvore faremos uso do seguinte comando: 1ldapadd -h localhost -x -D cn=admin,dc=secure,dc=inf,dc=br -w senha -f grupos.ldif Para a inserção de usuários usaremos o arquivo usuarios.ldif descrito abaixo: 12345678910111213141516171819202122232425262728dn: uid=ronaldo,ou=ti,ou=usuarios,dc=secure,dc=inf,dc=bruid: ronaldocn: Ronaldo Meneguitesn: MeneguiteobjectClass: inetOrgPerson objectClass: posixAccount homeDirectory: /home/ronaldo loginShell: /bin/bash uidNumber: 1000gidNumber: 1000userPassword: {SSHA}sZmrhpFA7XTkVGVrljx7QiUqU8kFLMlo dn: cn=replicator,dc=secure,dc=inf,dc=brcn: replicatorobjectClass: simpleSecurityObjectobjectClass: organizationalRoleuserPassword: {SSHA}sZmrhpFA7XTkVGVrljx7QiUqU8kFLMlo description: LDAP Replicatordn: uid=estagio,ou=ti,ou=usuarios,dc=secure,dc=inf,dc=br uid: estagiocn: Estagiariosn: EstagiaroobjectClass: inetOrgPersonobjectClass: posixAccounthomeDirectory: /home/estagiologinShell: /bin/bashuidNumber: 1002gidNumber: 1000userPassword: {SSHA}sZmrhpFA7XTkVGVrljx7QiUqU8kFLMlo Para inserirmos esses grupos em nossa árvore faremos uso do seguinte comando: 1ldapadd -x -D cn=admin,dc=secure,dc=inf,dc=br -w senha -f usuarios.ldif Com o comando abaixo, podemos verificar se os grupos e usuários foram inseridos com sucesso. 1ldapsearch –x –b =secure,dc=inf,dc=br Com sucesso nos comandos acima, foi listado todo o conteúdo do diretório LDAP inclusive com os grupos e usuários inseridos anteriormente, tendo assim em mãos um servidor OpenLDAP configurado e pronto para usar. Para o segundo servidor o serv2.secure.inf.br foi efetuado o mesmo procedimento de instalação do serviço, não inserindo os usuários e grupos como no servidor principal, visto que os mesmos serão replicados quando configurarmos a replicação. Ativando suporte a criptografiaO servidor OpenLDAP suporta trabalhar com dois esquemas de criptografia sendo eles, o SSL ou TLS onde a diferença entre eles é que o SSL por ser uma camada de segurança, tem sua porta alterada para uma porta diferente da padrão, a 389, enquanto no TLS, a criptografia é feita na camada de transporte, o que propicia ser ativado sem alteração da porta do serviço. A resistência da criptografia quando em ataque, tanto na SSL quanto do TLS ébasicamente a mesma, visto que ambos são fornecidos pelo OpenSSL. Para este estudo de caso utilizaremos o TLS como método de criptografia. Ativando suporte a TLSPara ativar o suporte a TLS será usado a seguinte sequência de comandos: 123456789101112131415161718192021# Instalar o OpenSSLapt-get install openssl# Criar o diretório para armazenamento da chavemkdir /etc/ldap/tlscd /etc/ldap/tls# Criando a agência certificadora/usr/lib/ssl/misc/CA.sh –newca# Criando certificado do servidor:openssl req –new –nodes –keyout newreq.pen –out newreq.pem# Assinando o certificado do servidor com o da agência certificadora criada:/usr/lib/ssl/misc/CA.sh –sign# Alterando os nomes dos certificados gerados para facilitar a identificação:mv newcert.pem srvcert.pem mv newreq.pem srvkey.pem# Copiando o certificado da agência certificadora para o mesmo diretórios dos outros: cp demoCA/cacert.pem . Ativando TLS no OpenLDAPPara ativarmos o suporte a TLS no OpenLDAP devemos adicionar no arquivo /etc/ldap/slapd.conf as seguintes linhas logo após a inclusão dos schemas: 123TLSCertificateFile /etc/ldap/tls/srvcert.pemTLSCertificateKeyFile /etc/ldap/tls/srvkey.pemTLSCACertificateFile /etc/ldap/tls/cacert.pem Após a inserção das linhas acima mencionadas reiniciamos o servidor com o seguinte comando: 1/etc/init.d/slapd restart Para testarmos a comunicação criptografada, prosseguimos com a configuração do cliente LDAP, sendo ele instalado junto com o servidor, de maneira que o mesmo possa utilizar TLS, bastando para isso inserirmos no final do arquivo a seguinte linha: 1TLS_CACERT /etc/ldap/tls/cacert.pem Para visualizamos o funcionamento da criptografia, faremos uso do seguinte comando: 1ldapsearch –x –ZZ Para futuro uso copiaremos os certificados gerados para a pasta /etc/ldap/tls/ do serv2.secure.inf.br. ReplicaçãoComo descrito anteriormente existe hoje duas maneiras de fazermos uma replicação utilizando o OpenLDAP , uma é usando o slurpd e outra usando o syncrepl, porém o slurpd foi descontinuado na versão 2.4 do OpenLDAP , além de possuir uma série de limitações. Assim foi definido a utilização do syncrepl. Configurando o serv1.secure.inf.br (MASTER)Foi iniciado a configuração do servidor Master alterando o arquivo de configuração /etc/ldap/slapd.conf, alterando o parâmetro modulepath para syncprov e inserindo as seguintes linhas logo abaixo da opção “índex”: 123overlay syncprov syncprov-checkpoint 100 10 syncprov-sessionlog 100 Inserimos também junto a ACL responsável pelos atributos de senha, userPassword e shadowLastChange a seguinte linha: 1by dn=\"cn=replicator,dc=secure,dc=inf,dc=br\" read Assim inserimos a permissão para o usuário replicador ler os campos de senha dos usuários, pois sem o acesso a leitura ele não conseguiria efetuar a replicação completa da base. Neste ponto foi reiniciado o servidor com o comando: 1/etc/init.d/slapd restart Configurando o serv2.secure.inf.br (SLAVE)Iniciamos a configuração do servidor slave editando o arquivo de configuração /etc/ldap/slapd.conf e removendo o caracter # antes da linha “rootdn”, e logo após inserimos as seguintes linhas abaixo de “index”: 12345678910111213syncrepl rid=1 provider=ldap://serv1.secure.inf.br:389 type=refreshAndPersist retry=\"5 + 5 +\" interval=00:00:00:10 searchbase=\"dc=secure,dc=inf,dc=br\" filter=\"(objectClass=*)\" scope=sub attrs=\"*\" schemachecking=on bindmethod=simple binddn=\"cn=replicator,dc=secure,dc=inf,dc=br\" credentials=doctum2009 Após os procedimentos acima citamos, inserimos a linha “TLS_CACERT /etc/ldap/tls/cacert.pem” no aquivo /etc/ldap/ldap.conf , paramos o servidor OpenLDAP , removemos os diretórios e novamente iniciamos o servidor, para isso utilizamos as seguintes linhas de comando: 123/etc/init.d/slapd stop rm /var/lib/ldap/* /etc/init.d/slapd start Assim temos agora os dois servidores configurados, sendo que os dados do serv1.secure.inf.br estão sendo replicados para o servidor serv2.secure.inf.br, porém o mesmo não tem autoridade sobre os dados fazendo com que para alterarmos qualquer informação na base, teremos que fazer isso no serv1.secure.inf.br e essa atualização será replicada de imediato para o serv2.secure.inf.br. CONCLUSÃOEste trabalho tem por objetivo apresentar um protocolo extremamente importante quando falamos em centralização de aplicativos conectados em rede, o LDAP, demonstrando que este é uma ótima solução por contar com uma arquitetura distribuída, métodos nativos de segurança, contar com padrão aberto, internacionalização e suporte ao ipv6, além de diversas outras funcionalidades. Também foi demonstrado nesse trabalho o quanto flexível o protocolo LDAP pode ser, possibilitando uma gama de possibilidades para o uso do mesmo. Este foi focado na solução livre OpenLDAP , a qual, apresentou características compatíveis com ferramentas proprietárias e, em alguns casos, até mesmo os superando, isso sem a necessidade de desprendermos recursos para aquisição de licenças. Acredita-se que este servirá como instrumento de referência para estudantes e profissionais da área de tecnologia que pretendem se aprofundar nesse protocolo, que a longa data é utilizado, massivamente, em empresas de médio a grande porte de todo o mundo. REFERÊNCIAS BIBLIOGRÁFICASBARTH, D. G.; SIEWERT, V. C. Conceituação de DNS. Disponível em:http://artigocientifico.uol.com.br/uploads/artc_1148560980_24.pdf. Acesso em: jun. 2009. CARTER, G.. LDAP Administração de Sistemas. Rio de Janeiro: Alta Books, 2009. FADEL, D.. Criptografia RSA. Disponível em: < http://www.ime.unicamp.br/~ftorres/ENSINO/MONOGRAFIAS/desi_RSA.pdf>. Acesso em: nov. 2009. GOUVEIA, B.. LDAP para iniciantes. Disponível em: http://www.ldap.org.br/. Acesso em: jun. 2009. LOSANO, F.. Integração de Rede com Diretórios LDAP. Disponível em: http://www.revistadolinux.com.br/ed/025/assinantes/rede.php3. Acesso em: mar. 2009. NOVELL. Novell Documentation: Novel Audit 2.0. Disponível em: . Acesso em: abr. 2009. OPENLDAP FOUNDATION. OpenLDAP Software 2.4 Administrator’s Guide. Disponível em: http://www.openldap.org/doc/admin24/. Acesso em: mai. 2009. SANTANA, F.. Instalação do Active Directory. Disponível em: http://www.fabianosantana.com.br/windows-2000/287-ad. Acesso em: mai 2009. TRIGO, C. H. OpenLDAP: Uma abordagem integrada. São Paulo: Novatec Editora, 2007. TUTTLE, S. EHLENBERGER, A.; GORTHI, R. Understanding LDAP: Design and Implementation. Disponível em: http://www.redbooks.ibm.com/. Acesso em: abr. 2009.","link":"/2009/12/01/ldap-lightweight-directory-access-protocol/"},{"title":"Comentários destrutivos em aplicativos","text":"Olá pessoal, hoje vou tentar expor aqui algo infelizmente muito comum no dia a dia de desenvolvedores de aplicativos, em especial os independentes onde esse tipo de ação tem maior impacto, os comentários destrutivos em uma loja de aplicativos. Este problema não se limita apenas ao contexto de aplicativos, podendo extrapolar para muitas áreas como a própria venda de produtos online, porém nesse artigo irei foca exclusivamente nesse contexto. Big Techs tem acostumado usuários a serem o produto, disponibilizando tudo sem uma cobrança direta e passando uma percepção para quem as usa ser “de graça”. Não é o intuito desse artigo entrar no mérito desta discussão, mais saiba, se não paga diretamente por uma solução, muito provavelmente é porque você e seus dados são o produto, nenhuma empresa disponibiliza uma aplicação comercial sem o intuito de ganhar dinheiro com a mesma, quem nunca pesquisou por um produto como televisão no google por exemplo, e depois em todo site que abre só vê anuncio de televisões? Essa percepção de ser tudo “grátis” está criando uma geração que espera ter tudo que precisa sem ter que disponibilizar nem mesmo alguns centavos para financiar quem vai produzir a solução, é comum encontrar sites especializados em disponibilizar aplicativos que seriam pagos nas lojas de aplicativos oficiais, muitas vezes alguns centavos, sem nenhum custo. Muitos preferem se expor e sair da segurança da loja do ecossistema e financiar o aplicativo que tanto gosta, para economizar R$ 0,99, expondo seu celular, seus dados e se colocando em risco por tão pouco. Voltando ao assunto do texto, com este cenário em mente, podemos perceber o quão difícil é para desenvolvedores independentes e mesmo empresas que não possuam o acesso e capital que as grandes empresas de tecnologia tem, criar e manter um projeto direcionado para o consumidor final ativo e principalmente rentável. Se só esses desafios não fossem suficientes, temos que administrar comentários como os abaixo, que desmerecem todo esse trabalho, desqualificam uma equipe por não atender o que esperam, ainda que não estejam dispostos a pagar para ter o que precisam. Adoraria que esses comentários fossem uma excessão, porém não são, são extremamente comuns e recheados de contradições como “Ótimo app! Se tivesse …”, “Se tivesse X seria perfeito”, etc.. Oi? Como um aplicativo que é ótimo ou quase perfeito, segundo eles, pode receber uma qualificação de apenas uma estrela? Não me entendam mal, não espero uma avaliação positiva em todos os comentários, ao contrário, muitas vezes uma qualificação negativa nos lança um feedback que pode nos ajudar a evoluir muito a solução, mas é necessário ter coerência. Uma avaliação como estas, além de prejudicar muito o impulsionamento do aplicativo dentro da loja, são um completo balde de água fria em uma equipe que está trabalhando para resolver um problema, e considerando que está usando a solução, provavelmente o seu problema. Desta forma vou propor alguns pontos que seriam incriveis se todos que forem avaliar um aplicativo usassem. 1: Sejam coerentes na avaliação, a variação existe para ser usada, não é 8 ou 80, se não gosta de tudo que está lá pese o que tem que gosta e o que não gosta e de uma avaliação justa entre esses pontos; 2: Não gostar de algum ponto ou escolha de uma funcionalidade é normal e nem sempre uma solução vai agradar a todos, mas não deixe de dar o feedback, descreva seu ponto de vista, e como a mudança poderia ser favorável. 3: Nenhuma solução nasce completa, as funcionalidades são acrescentadas muitas vezes aos poucos, se sente falta de algo, passe o feedback para a equipe, muitas vezes ter esse retorno pode fazer a equipe perceber qual funcionalidade é mais ou menos importante e priorizar de forma adequada. 4: Nunca, jamais, de forma nenhuma, vinculem um ponto, uma funcionalidade a uma avaliação positiva. É ridículo ler um comentário com algo como “Quando incluírem a funcionalidade X eu dou 5 estrelas”. Seguindo esses pontos básicos, certamente teriamos um ecosistema muito mais saudável e propício a inovação. Mas digam aí, como tem sido a experiência de vocês? Já passaram ou viram algo assim?","link":"/2021/01/16/comentarios-destrutivos-em-aplicativos/"},{"title":"Compilando Typescript/Javascript para executáveis nativos com Deno","text":"O Deno tem evoluído muito rápido, e cada dia vem trazendo novas features que o deixam cada vez mais interessante, na versão 1.6 que saiu dia 8/12/2020 foi introduzida, entre outras features (como o suporte experimental aos novos macs com chips M1), essa nova funcionalidade que permite gerar um executável nativo standalone and self contained. Algo semelhante ao que libs como nexe e pkg fazem com o node, porém de forma integrada. Na versão seguinte, a 1.7 lançada em 19/01/2021, tivemos ainda algumas melhorias que incluem a possibilidade de fazer compilação cruzada de qualquer arquitetura estável suportada (Windows x64, MacOS x64 e Linux x64) para qualquer outra arquitetura, o que possibilita criar binários para Windows ou Mac a partir de uma máquina linux por exemplo. Outra feature muito interessante incluída na nova versão foi a flag “—lite” que pode gerar um executável entre 40/60% menor. Caso já tenha instalado versões anteriores do Deno pode fazer a atualização usando o comando \"deno upgrade\", porém caso esteja fazendo a instalação pela primeira vez, pode usar os seguintes comandos a depender do seu sistema operacional: Using Shell (macOS and Linux)1curl -fsSL https://deno.land/x/install/install.sh | sh Using PowerShell (Windows)1iwr https://deno.land/x/install/install.ps1 -useb | iex Using homebrew (MacOS)1brew install deno Using Scoop (Windows)1scoop install deno Using Chocolatey (Windows)1choco install deno Build from source using cargo1cargo install deno Um dos pontos que me agradam bastante no ecossistema do Deno é que ele busca, provavelmente inspirado em outros ecosistemas como o de Go ou Rust, prover todo um pacote nativo completo com ferramentas para formatação (deno fmt) e Lint (deno lint) por exemplo, com a versão 1.6 foi incluído o compile, que é o responsável por prover essa nova feature, tema deste artigo. Como usar o compilePara testar a ferramentas nada melhor que um hello world, assim: 1echo \"console.log('Hello World')\" > index.ts Para compilar esse arquivo é bem simples, basta executar o seguinte comando: 1deno compile --unstable index.ts O executável gerado por essa compilação: -rwxrwxrwx 1 ronaldo staff 44M Feb 4 10:26 compile Porém usando a nova flag “—lite” da versão 1.7 o executável é reduzido consideravelmente como pode ser visto abaixo: 1deno compile --unstable --lite index.ts Gerando o seguinte executável: -rwxrwxrwx 1 ronaldo staff 30M Feb 4 10:24 compile Comparando com soluções similares do ecosistema nodeComo referência vamos fazer o mesmo teste usando uma solução similar no node o pkg. Para instalar o pkg basta executar:1npm install -g pkg Gerando o mesmo arquivo .js para compilação1echo \"console.log('Hello World')\" > index.js Para compilar1pkg --targets node14-macos-x64 index.js --output compile O que resulta em um executável consideravelmente maior, mesmo se comparado a versão completa do Deno, como pode ser visto abaixo: -rwxr-xr-x 1 ronaldo staff 69M Feb 4 10:40 compile Nem só de Hello World vive uma solução :)A própria equipe do Deno disponibiliza um exemplo de aplicação que sobe um servidor http e pode ser uma boa referência, para compilar e gerar o executavel, basta executar os comandos: Compilando1deno compile --unstable --allow-net=0.0.0.0:8080 --allow-read=. https://deno.land/std@0.83.0/http/file_server.ts --cors --port 8080 Executando1./file_server HTTP server listening on http://0.0.0.0:8080/ Limitações atuais do compile do DenoO compile do deno ainda não é uma ferramenta perfeita, e tem limitações claras que precisam ser levadas em consideração ao usá-la, algumas delas já estão mapeadas e tendem a ser resolvidas em breve, segue abaixo algumas das limitações: Os Web Workers não são suportados atualmente. #8654 . Você não pode incluir código dinamicamente com importação dinâmica. #8655 . A personalização de sinalizadores V8 e permissões de sandbox não é possível atualmente. #8656 . ConclusãoO Deno tem trilhado um caminho muito interessante, o compile é só mais uma pimenta que com certeza agrega muito ao ecosistema que vem sendo criado ao entorno do Deno. É evidente que ele ainda tem um chão para amadurecer e ganhar “músculos”, mas me agrada os caminhos e as possibilidades que abrem com essa visão do time core do Deno. E ai o que acharam dessa nova feature?","link":"/2021/02/04/compilando-typescript-javascript-para-executaveis-nativos-com-deno/"},{"title":"Principais comandos do NPM","text":"É inegável a popularidade do NodeJs em quase todas as áreas do desenvolvimento, usa-se node para quase tudo, desde soluções mobile com Reat Native e IONIC, backend de aplicações, ferramentas de linha de comando, até a profunda dependências nos fluxos de desenvolvimento de frontends ricos com VueJs, React e Angular, assim conhecer o entorno de seu ecosistema é essencial. Com isso em mente, resolvi escrever este artigo e deixar um pouco mais mastigado e acessível os principais comandos de seu gerenciador de pacote oficial, o NPM (Node Package Manager), usado extensivamente no dia a dia para gerenciar as dependências do projeto. Atalhos mais comumns Comando Atalho install i list ls test t –global -g –save -S –save-dev -D Principais Comandosnpm help {COMMAND}Para se obter o manual do comando npm install {MODULE}Ou apenas “npm i {MODULE}“, é usado para instalar dependências Executar apenas “npm install“ instala todas as dependências configuradas no arquivos package.json; Ao usar a flag “--production“ serão instaladas apenas depedências de produção; npm remove {MODULE}Remove um módulo previamente instalado npm initComando usado para iniciar um projeto novo na pasta atual, se utilizado a flag “-y“ assim “npm init -y“ será gerado uma inicialização com os parâmetros default sem questionamentos. npm ciUsado para o deploy de aplicações instalando todas as dependência definidas package.json e versões do package-lock.json Caracteristicas importantes: Caso a pasta node_modules exista a mesma será removida e recriada na sequência Esse comando não altera os arquivos package.json e package-lock.json É requerido que na pasta se tenha o arquivo package-lock.json ou npm-shrinkwrap.json npm outdatedRetorna a lista de dependências desatualizadas mostrando a versão mais recente suportada pela configuração no package.json e a ultima versão npm updateAtualiza projetos para a ultima versão respeitando o package.jsonPode-se atualizar um único módulo executando “npm update {MODULE}” npm version {COMMAND}Manipula a versão atual do projeto e adiciona tags no projeto gitComandos disponíveis: {VERSION} | major | minor | patch | premajor | preminor | prepatch | prereleaseEx: npm version patch ou npm version 1.0.20 npm auditExecuta uma auditoria no projeto para identificar se existe alguma dependência com vulnerabilidade conhecidaAo utilizar a flag --fix é feita uma tentativa de corrigir o problema de forma automática. npm listOu apenas “npm ls”, lista todas as dependências do projetoCom a flag “–depth X” podemos ver uma arvore de dependências onde X é a profundidade que se deseja npm shrinkwrapComando usado para travar a versão das dependências do seu projetoAo executar esse comando será criado o arquivo npm-shrinkwrap.json que servirá de base para instalação das dependências npm adduser {USERNAME}Adiciona um novo usuário no registre para permitir o envio de pacotes para o mesmo npm publishPublica o módulo atual no registre configurado npm home {MODULE}Abre a página do projeto npm repo {MODULE}Abre o repositório de código do projeto`","link":"/2021/07/05/principais-comandos-do-npm/"},{"title":"Publicando uma aplicação NodeJs na Digital Ocean","text":"Enfim, depois de todo esforço para desenvolver aquela aplicação, chegou a hora de subir ela em produção. Escolher onde e como disponibilizar essa solução para seus clientes em potencial pode ser determinante para o sucesso do projeto. Com isso em mente, temos ótimas soluções no mercado, entre as principais estão: Amazon AWS, Google Cloud, Microsoft Azure, e&nbsp;a que será foco desse artigo, Digital Ocean. Cada um destes provedores de serviço tem características bem peculiares que podem ou não aderir ao objetivo do projeto, não estando no escopo do artigo discorrer sobre as características de cada uma, porém é importante destacar que a Digital Ocean atende bem a muitos cenários, em especial aqueles que não possuem um grande fluxo de caixa inicial e precisam ter mais controle sobre o custo total de infraestrutura com um valor mais previsível, isso acontece pois lá é possível iniciar pequenas máquinas virtuais, denominadas “droplets” com um custo mensal a partir de US $ 5.00 por mês, na data que escrevo o artigo, com 1GB de memória, 1vCPU, 25 GB de SSD e 1TB de transferência, um valor bem razoável para o que entrega. Como exemplo, a aplicação que iremos utilizar no artigo é uma api muito simplificada disponível em https://github.com/meneguite/node-simple-api, que nos possibilita descrever com mais facilidade uma forma de levar uma aplicação a produção, usando para isso além do node e npm, o nginx como proxy reverso e o pm2 como supervisor de instancia da aplicação. Configuração do DropletPara iniciarmos é necessário uma conta na Digital Ocean, caso ainda não tenha uma, crie a por esse link https://m.do.co/c/2e1c3d77e32b, o cadastro é muito simplificado, e seguindo o link irá receber US $100 de bônus, o que possibilitara testar completamente a solução proposta por este artigo. Com a conta criada vamos criar nosso primeiro droplet: Selecionaremos o plano que melhor se enquadra na nossa necessidade: É possível incluir novos blocos de storage ao droplet, extendendo a capacidade de armazenamento base do plano, porém para esse exemplo não será necessário. Selecionar a região do datacenter: Selecionar o método de autenticação: Para a autenticação recomendo fortemente que crie uma chave ssh, se já não possuir uma, isso será um bom incremento de segurança em sua instancia. É possível também, quando necessário, anexar ao droplet um agendamento automático de backup. Tudo correndo bem, visualizará uma tela semelhante a demonstrada abaixo, com nome da instancia e o ip externo atribuído a mesma: Para acessar nossa instancia podemos utilizar o comando: 123ssh root@161.35.10.18 # Onde 161.35.10.18 é o ip externo disponibilizado pelo Digital Ocean Instalando e Configurando as dependências para o projetoConfigurando um novo usuário comumComo pode-se perceber ao acessar nosso novo droplet, o usuário base criado é o root, que possui privilégios muito elevados, sendo fortemente desencorajado a rodar uma aplicação com esse usuário, assim nossa primeira configuração será criar um novo usuário com privilégios menores para rodar nossa aplicação. 12345678910111213141516171819adduser web# OutputAdding user `web' ...Adding new group `web' (1000) ...Adding new user `web' (1000) with group `web' ...Creating home directory `/home/web' ...Copying files from `/etc/skel' ...New password:Retype new password:passwd: password updated successfullyChanging the user information for webEnter the new value, or press ENTER for the default Full Name []: Web Room Number []: Work Phone []: Home Phone []: Other []:Is the information correct? [Y/n] y A nova conta criada agora é um usuário com privilégios básicos, porém em alguns momentos precisaremos fazer algumas tarefas administrativas, caso não queira fazer a troca de usuário sempre que necessário executar essas tarefas, pode-se incluir esse novo usuário no grupo “sudo”, permitindo que o usuário comum tenha acesso a comandos administrativos mediante ao uso do prefixo “sudo”, para isso execute o seguinte comando: 1usermod -aG sudo web Liberando acesso via ssh para o novo usuárioCaso esteja usando uma chave ssh, como sugerido acima,&nbsp;para acesso ao servidor, será necessário adicionar uma chave autorizada para o usuário também, permitindo que o mesmo consiga logar no sistema. Em sistemas críticos é extremamente importante não usar a mesma chave para ambos os acessos, porém para este exemplo iremos copiar a mesma autorização do usuário root para simplificar o processo. 1rsync --archive --chown=web:web /root/.ssh /home/web Com esse comando será copiado os arquivos necessários para acesso, mantendo as permissões adequadas. Agora podemos nos desconectar via root acessando novamente o servidor usando nosso novo usuário com o comando: 1ssh web@161.35.10.18 Ativando e configurando algumas regras básicas de acesso no FirewallO linux possui muitas ferramentas interessantes e poderosas para manter a integridade do servidor restringindo o acesso ao mínimo necessário, para esse exemplo iremos utilizar um firewall que já vem incluso no ubuntu e que a principio atende bem nossa demanda, visto que simplifica muito a criação e&nbsp;administração das regras de acesso ao servidor usando perfis de aplicação registradas na instalação de alguns softwares. Para visualizar os perfis disponíveis no sistema: 12345sudo ufw app list# OutputAvailable applications: OpenSSH Antes de ativar o firewall, muita atenção para garantir que as conexões via SSH estão habilitadas, para isso execute o seguinte comando: 12345sudo ufw allow OpenSSH# OutputRules updatedRules updated (v6) Depois disso podemos ativar nosso firewall&nbsp;usando seguinte comando: 12345sudo ufw enable# OutputCommand may disrupt existing ssh connections. Proceed with operation (y|n)? yFirewall is active and enabled on system startup Para visualizar as regras ativas no sistema execute o comando: 123456789sudo ufw status# OutputStatus: activeTo Action From-- ------ ----OpenSSH ALLOW AnywhereOpenSSH (v6) ALLOW Anywhere (v6) Instalando o NodeJsPara instalarmos o node iremos usar os repositórios disponibilizados via PPA com os seguintes comandos: 12cd ~curl -sL https://deb.nodesource.com/setup_14.x | sudo -E bash - Após a conclusão dos comandos acima, estamos aptos agora a proceder com a instalação do node. Repare porém, que no segundo comando eu defino a versão 14, a LTS no momento que escrevo este artigo, porém provavelmente só ajustar a versão mais atual será o suficiente para fazer a instalação da versão correta que precisa para rodar sua aplicação. Para instalar agora o nodejs basta executar o seguinte comando: 1sudo apt install nodejs build-essential A instalação do pacote “build-essential” é opcional neste exemplo, porém muito provavelmente vai precisar dele no dia a dia no servidor com aplicações node. Para verificar a instalação podemos executar o seguinte comando: 1234node -v# Outputv14.17.2 1234npm -v# Output6.14.13 Instalando e configurando o PM2O PM2 é um gerenciador de processos para aplicativos node.js que nos possibilitará gerenciar, escalonar e manter o aplicativos sempre disponível. A instalação do PM2 é muito simples e feita diretamente usando o NPM com o comando abaixo: 1sudo npm install pm2@latest -g Para verificar se a instalação foi feita com sucesso use o comando: 1234pm2 --version# Output5.1.0 Agora chegou o momento de obtermos nossa aplicação para seguirmos com a configuração, para simplificar esse artigo removendo complexidades de integrar esse fluxo em um servidor de CI/CD por exemplo, vou apenas simular que já tenho os arquivos no servidor, baixando os mesmos do repositório via GIT com a seguinte sequência de comandos: 123cd ~git clone -b master https://github.com/meneguite/node-simple-api.git node-simple-apicd node-simple-api Para visualizar os arquivos disponibilizados: 12345678910ls -la# Outputtotal 24drwxrwxr-x 3 web web 4096 Jul 12 22:01 .drwxr-xr-x 8 web web 4096 Jul 12 22:01 ..-rw-rw-r-- 1 web web 291 Jul 12 22:01 ecosystem.config.jsdrwxrwxr-x 8 web web 4096 Jul 12 22:01 .git-rw-rw-r-- 1 web web 1627 Jul 12 22:01 index.js-rw-rw-r-- 1 web web 594 Jul 12 22:01 README.md Ps: No droplet criado com ubuntu 20 já temos o git instalado por padrão, porém caso ainda não o tenha, basta executar o seguinte comando “sudo apt install -y git” para instalar. Com os arquivos em mãos podemos dar inicio a configuração da nossa aplicação no PM2, essa configuração pode ser feita parametrizando o pm2 via linha de comando, porém para simplificar disponibilizei no repositório um arquivo “ecosystem.config.js” que já possui todos os parâmetros necessários para configurar a aplicação. O formato do arquivo ecosystem é o seguinte: 12345678910111213141516module.exports = { apps: [ { name: 'node-simple-api', script: './index.js', instances: 2, exec_mode: 'cluster', merge_logs: true, env_production: { DEBUG: false, NODE_ENV: 'production', APP_PORT: 3000, }, }, ],}; Para iniciarmos nossa aplicação usando esse nosso arquivo e garantir que o mesmo seja iniciado automaticamente sempre que o PM2 iniciar devemos executar o seguinte comando: 1234567891011pm2 start ecosystem.config.js --env production &amp;&amp; pm2 save# Output[PM2] Applying action restartProcessId on app [node-simple-api](ids: [ 0, 1 ])[PM2] [node-simple-api](0) ✓[PM2] [node-simple-api](1) ✓┌─────┬────────────────────┬─────────────┬─────────┬─────────┬──────────┬────────┬──────┬───────────┬──────────┬──────────┬──────────┬──────────┐│ id │ name │ namespace │ version │ mode │ pid │ uptime │ ↺ │ status │ cpu │ mem │ user │ watching │├─────┼────────────────────┼─────────────┼─────────┼─────────┼──────────┼────────┼──────┼───────────┼──────────┼──────────┼──────────┼──────────┤│ 0 │ node-simple-api │ default │ N/A │ cluster │ 14177 │ 0s │ 1 │ online │ 0% │ 37.8mb │ web │ disabled ││ 1 │ node-simple-api │ default │ N/A │ cluster │ 14189 │ 0s │ 1 │ online │ 0% │ 30.8mb │ web │ disabled │ Assim já temos nossa aplicação rodando na porta 3000 localmente, como podemos verificar abaixo: 1234curl http://127.0.0.1:3000/users# Output[{\"id\":1,\"name\":\"Nanna Pedersen\",\"email\":\"nanna.pedersen@example.com\"},{\"id\":2,\"name\":\"Sarah Oliver\",\"email\":\"sarah.oliver@example.com\"},{\"id\":3,\"name\":\"Hector Guerrero\",\"email\":\"hector.guerrero@example.com\"},{\"id\":4,\"name\":\"Noah Poulsen\",\"email\":\"noah.poulsen@example.com\"}] O PM2 é muito flexível e poderoso, nesse momento já temos disponível nossa aplicação e podemos escalar ela sem nenhuma dificuldade, por exemplo, para subir para 4 o número de instancias disponíveis, podemos executar o comando: 12345678910111213pm2 scale node-simple-api 4# Output[PM2] Scaling up application[PM2] Scaling up application┌─────┬────────────────────┬─────────────┬─────────┬─────────┬──────────┬────────┬──────┬───────────┬──────────┬──────────┬──────────┬──────────┐│ id │ name │ namespace │ version │ mode │ pid │ uptime │ ↺ │ status │ cpu │ mem │ user │ watching │├─────┼────────────────────┼─────────────┼─────────┼─────────┼──────────┼────────┼──────┼───────────┼──────────┼──────────┼──────────┼──────────┤│ 0 │ node-simple-api │ default │ N/A │ cluster │ 14177 │ 4m │ 1 │ online │ 0% │ 39.8mb │ web │ disabled ││ 1 │ node-simple-api │ default │ N/A │ cluster │ 14189 │ 4m │ 1 │ online │ 0% │ 38.6mb │ web │ disabled ││ 2 │ node-simple-api │ default │ N/A │ cluster │ 14238 │ 0s │ 0 │ online │ 0% │ 35.4mb │ web │ disabled ││ 3 │ node-simple-api │ default │ N/A │ cluster │ 14245 │ 0s │ 0 │ online │ 0% │ 30.3mb │ web │ disabled │└─────┴────────────────────┴─────────────┴─────────┴─────────┴──────────┴────────┴──────┴───────────┴──────────┴──────────┴──────────┴──────────┘ Mais informações e exemplos de uso do PM2 podem ser visualizados na documentação oficial disponível em https://pm2.keymetrics.io/docs/usage/quick-start/ Para configurarmos o PM2 para iniciar junto com o sistema operacional é bem simples, e o próprio PM2 já nos facilita com o comando “pm2 startup” 123456pm2 startup# Output[PM2] Init System found: systemd[PM2] To setup the Startup Script, copy/paste the following command:sudo env PATH=$PATH:/usr/bin /usr/lib/node_modules/pm2/bin/pm2 startup systemd -u web --hp /home/web 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081sudo env PATH=$PATH:/usr/bin /usr/lib/node_modules/pm2/bin/pm2 startup systemd -u web --hp /home/web# Output -------------__/\\\\\\\\\\\\\\\\\\\\\\\\\\____/\\\\\\\\____________/\\\\\\\\____/\\\\\\\\\\\\\\\\\\_____ _\\/\\\\\\/////////\\\\\\_\\/\\\\\\\\\\\\________/\\\\\\\\\\\\__/\\\\\\///////\\\\\\___ _\\/\\\\\\_______\\/\\\\\\_\\/\\\\\\//\\\\\\____/\\\\\\//\\\\\\_\\///______\\//\\\\\\__ _\\/\\\\\\\\\\\\\\\\\\\\\\\\\\/__\\/\\\\\\\\///\\\\\\/\\\\\\/_\\/\\\\\\___________/\\\\\\/___ _\\/\\\\\\/////////____\\/\\\\\\__\\///\\\\\\/___\\/\\\\\\________/\\\\\\//_____ _\\/\\\\\\_____________\\/\\\\\\____\\///_____\\/\\\\\\_____/\\\\\\//________ _\\/\\\\\\_____________\\/\\\\\\_____________\\/\\\\\\___/\\\\\\/___________ _\\/\\\\\\_____________\\/\\\\\\_____________\\/\\\\\\__/\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\_ _\\///______________\\///______________\\///__\\///////////////__ Runtime Edition PM2 is a Production Process Manager for Node.js applications with a built-in Load Balancer. Start and Daemonize any application: $ pm2 start app.js Load Balance 4 instances of api.js: $ pm2 start api.js -i 4 Monitor in production: $ pm2 monitor Make pm2 auto-boot at server restart: $ pm2 startup To go further checkout: http://pm2.io/ -------------[PM2] Init System found: systemdPlatform systemdTemplate[Unit]Description=PM2 process managerDocumentation=https://pm2.keymetrics.io/After=network.target[Service]Type=forkingUser=webLimitNOFILE=infinityLimitNPROC=infinityLimitCORE=infinityEnvironment=PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/usr/bin:/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/binEnvironment=PM2_HOME=/home/web/.pm2PIDFile=/home/web/.pm2/pm2.pidRestart=on-failureExecStart=/usr/lib/node_modules/pm2/bin/pm2 resurrectExecReload=/usr/lib/node_modules/pm2/bin/pm2 reload allExecStop=/usr/lib/node_modules/pm2/bin/pm2 kill[Install]WantedBy=multi-user.targetTarget path/etc/systemd/system/pm2-web.serviceCommand list[ 'systemctl enable pm2-web' ][PM2] Writing init configuration in /etc/systemd/system/pm2-web.service[PM2] Making script booting at startup...[PM2] [-] Executing: systemctl enable pm2-web...Created symlink /etc/systemd/system/multi-user.target.wants/pm2-web.service → /etc/systemd/system/pm2-web.service.[PM2] [v] Command successfully executed.+---------------------------------------+[PM2] Freeze a process list on reboot via:$ pm2 save[PM2] Remove init script via:$ pm2 unstartup systemd 12345pm2 save# Output[PM2] Saving current process list...[PM2] Successfully saved in /home/web/.pm2/dump.pm2 Instalando e configurando o nginx como proxy reversoPara instalarmos o nginx via PPA devemos executar os seguintes comandos: 12sudo add-apt-repository ppa:nginx/stable -ysudo apt install -y nginx Podemos verificar se a instalação foi finalizada com sucesso com o seguinte comando: 12345678910111213141516sudo service nginx status# Output● nginx.service - A high performance web server and a reverse proxy server Loaded: loaded (/lib/systemd/system/nginx.service; enabled; vendor preset: enabled) Active: active (running) since Tue 2021-07-13 13:56:54 UTC; 18min ago Docs: man:nginx(8) Main PID: 749 (nginx) Tasks: 2 (limit: 1136) Memory: 11.0M CGroup: /system.slice/nginx.service ├─749 nginx: master process /usr/sbin/nginx -g daemon on; master_process on; └─752 nginx: worker processJul 13 13:56:53 node-simple-api systemd[1]: Starting A high performance web server and a reverse proxy server...Jul 13 13:56:54 node-simple-api systemd[1]: Started A high performance web server and a reverse proxy server. Tendo isso em mãos vamos iniciar a configuração de um endereço DNS de entrada, para o teste irei usar o endereço “simple-api.meneguite.com”, para isso iremos criar o arquivo “/etc/nginx/sites-available/simple-api.meneguite.com.conf” com o seguinte conteúdo: 123456789101112131415161718192021222324252627282930313233343536server { listen 80 default_server; listen [::]:80 default_server; server_tokens off; server_name simple-api.meneguite.com; # Aditional Security Headers # ref: https://developer.mozilla.org/en-US/docs/Security/HTTP_Strict_Transport_Security add_header Strict-Transport-Security \"max-age=31536000; includeSubDomains\"; # ref: https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/X-Frame-Options add_header X-Frame-Options DENY always; # ref: https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/X-Content-Type-Options add_header X-Content-Type-Options nosniff always; # ref: https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/X-XSS-Protection add_header X-Xss-Protection \"1; mode=block\" always; location / { proxy_http_version 1.1; proxy_redirect off; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarde $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; proxy_set_header X-NginX-Proxy true; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection \"upgrade\"; proxy_cache_bypass $http_upgrade; proxy_buffering off; proxy_pass \"http://127.0.0.1:3000\"; }} Remover o arquivo default de configuração do nginx: 1sudo rm /etc/nginx/sites-enabled/default Criar um link simbólico para habilitar o novo arquivo de configuração: 1sudo ln -s /etc/nginx/sites-available/simple-api.meneguite.com.conf /etc/nginx/sites-enabled/simple-api.meneguite.com.conf Para testarmos e verificarmos se os arquivos e configurações estão ok antes de reiniciar os serviços execute o seguinte comando: 1234sudo service nginx configtest# Output * Testing nginx configuration [ OK ] Tudo certo e configurado podemos fazer o reload do nginx para que o mesmo leia as configurações novamente e ative corretamente os serviços: 1sudo service nginx reload Se tudo estiver corrido como esperado até aqui, ao executar o comando abaixo já poderemos ver nossa api funcionando já por trás do proxy reverso do nginx. 1234curl http://127.0.0.1/users# Output[{\"id\":1,\"name\":\"Nanna Pedersen\",\"email\":\"nanna.pedersen@example.com\"},{\"id\":2,\"name\":\"Sarah Oliver\",\"email\":\"sarah.oliver@example.com\"},{\"id\":3,\"name\":\"Hector Guerrero\",\"email\":\"hector.guerrero@example.com\"},{\"id\":4,\"name\":\"Noah Poulsen\",\"email\":\"noah.poulsen@example.com\"}] Tudo configurado e funcionando, porém se tentar acesso pelo browser receberá a seguinte mensagem: Isso acontece devido a ainda não termos feito a liberação da porta 80 de entrada necessária para acesso ao nginx, para fazer essa liberação devemos proceder com os seguintes comandos: Verificar os apps disponíveis: 12345678sudo ufw app list# OutputAvailable applications: Nginx Full Nginx HTTP Nginx HTTPS OpenSSH No nosso exemplo não configuramos os certificados para usarmos o https, assim vamos por hora liberar acesso apenas a porta http do ngix com o seguinte comando: 12345sudo ufw allow 'Nginx HTTP'# OutputRule addedRule added (v6) E enfim temos nosso serviço instalado e configurado: Nosso servidor está acessível externamente e com a aplicação clousterizada usando o PM2, porém não é tão legal acessar nosso serviço sem um https, muito menos pelo ip, para resolver isso podemos usar o serviço da cloudflare para nos prover um certificado https válido, este processo é bem simples e descrevo como fazer no artigo “Publicar um site com Github Pages e CloudFlare” acessível no endereço: https://meneguite.com/2018/11/10/publicar-um-site-com-github-pages-e-cloudflare Tudo pronto agora! Para o escopo deste artigo chegamos ao final, mas claro, sabemos que muitas outras necessidades vem ao criar uma aplicação, com essa configuração básica já consegue iniciar e validar pontos iniciais com um custo muito baixo, porém durante a trajetória provavelmente vai sentir a necessidade de um banco de dados, relacional ou não, e pode lançar outros droplets para resolver esta demanda, ou pode optar por usar um serviço disponibilizado pela própria Digital Ocean o “Managed Databases”, com esse serviço terceiriza toda a complexidade inicial de configurar e manter um servidor de banco de dados, para mais informações acesse “https://www.digitalocean.com/products/managed-databases/“. Outra demanda que muito provavelmente precisará em breve é trabalhar na resiliência da aplicação, ainda que muito estável, os servidores ou o próprio droplet pode parar de funcionar em algum momento, ou mesmo sua aplicação pode crescer e somente um único servidor deixar de ser suficiente, assim uma boa estratégia será incluir no meio do caminho um servidor de load balancer, serviço provido pela própria digital ocean, e lançar duas ou mais instancias do droplet que configuramos neste artigo, assim mesmo que tenha uma demanda maior ou um dos droplets tenha algum problema, nosso serviço continua a funcionar normalmente. Os códigos usados neste artigo estão disponíveis no repositório https://github.com/meneguite/node-simple-api.git, qualquer dúvida ou sugestão de melhora fique a vontade para deixar aqui abaixo nos comentários.","link":"/2021/07/13/publicando-uma-aplicacao-nodejs-na-digital-ocean/"},{"title":"Pipes - Comunicação entre processos","text":"Abstract: This article discusses about pipe mechanism. Pipe was created by Doug McIlroy, for Unix shell, and was named by analogy to fluid transports pipes. The central idea of pipe is to link a set of processes, in a way that each process’ output is the next process’s input. Resumo: Este artigo discorre brevemente sobre o mecanismo de PIPE. Criado pelo professor Doug McIlroy, para o shell do Unix, o mecanismo tem seu nome inspirado nos tubos(pipes), de transporte de fluídos. A idéia do PIPE é encadear um conjunto de processos, de modo que a saída de cada um seja a entrada do seguinte. Comunicação entre processosA comunicação entre processos, do inglês Inter-Process Communication (IPC), é o grupo de mecanismos ao qual permite que processos possam transferir informações entre si. Para a execução de um processo é pressuposto por parte do sistema operacional, entre outras coisas, a criação de um contexto de execução própria ao qual abstrai os processos dos componentes reais do sistema. Devido a esta “virtualização” dos recursos, o processo não tem conhecimento acerca dos outros processos e, como tal, não consegue trocar informação com os outros processos. PipesPipes unidirecionaisEsta é a forma mais divulgada de IPC. O Pipe unidirecional, ou “canalização”, é o redirecionamento da saída padrão de um programa para a entrada padrão de outro. É um recurso muito utilizado em sistemas Unix/Linux para tratar entradas e saídas de dados, um recurso muito versátil ao qual permite que seja redirecionado a entrada ou saída padrão de um determinado processo, para um segundo processo. Um exemplo de uso do pipe no linux seria: Figura 1 – Representação gráfica de uso do PIPE Pipes nomeados (FIFO)No contexto da computação, um pipe nomeado (também chamado de named pipe ou FIFO) é uma extensão do conceito de pipe do sistema Unix/Linux e dos seus métodos de comunicação entre processos. O mesmo conceito é encontrado no Microsoft Windows, apesar da sua semântica ser razoavelmente diferente. Um pipe tradicional (anônimo) dura somente o tempo de execução do processo e, por outro lado, o pipe nomeado persiste além da vida do processo e precisa ser “desligado” ou apagado quando não é mais usado. Os processos geralmente se conectam a um pipe nomeado (normalmente um arquivo) quando necessitam realizar alguma comunicação com outro processo (IPC). Pipe nomeado no Unix/LinuxAo contrário do pipe convencional, não nomeado e de console, um pipe nomeado é criado explicitamente utilizando-se os comandos mknod ou mkfifo. Dois processos podem utilizar este pipe através do seu nome. Por exemplo, pode-se criar um pipe e instruir o programa gzip para comprimir aquilo que é enviado (piped) para ele. 12mkfifo pipegzip -9 -c &lt; pipe &gt; destino Independentemente, em um processo separado, pode-se executar o comando 1cat origem &gt; pipe Esse comando comprimirá os dados do arquivo “origem” em um arquivo de nome “destino”. Pipe nomeado no WindowsNo Windows, o projeto do pipe nomeado foi influenciado pela comunicação cliente-servidor e, por isso, eles se parecem muito mais com sockets do que com operações convencionais de escrita e leitura. O Windows suporta um “modo passivo” para aplicações servidoras (comparável com os domain sockets do Unix). O Windows 95 suporta pipes nomeados em modo cliente que, no Windows NT, também podem atuar como servidores. O pipe nomeado pode ser acessado como um arquivo. As funções CreateFile, ReadFile, WriteFile e CloseHandler, da SDK Win32, podem ser utilizadas para abrir, ler, escrever e fechar um pipe. As funções da biblioteca C, como fopen, também podem ser utilizadas para operações em pipes nomeados, ao contrário do caso dos Windows Sockets, onde a comunicação em rede não foi implementada como operações padrão de I/O em arquivos. Não existe interface de linha de comando como nos sistemas Unix. O pipe nomeado não é permanente e não pode ser criado como um arquivo especial em sistemas de arquivo que permitam escrita, como no Unix. O pipe é volátil (é liberado após a última referência para ele ser fechada), é alocado na raiz do named pipe filesystem (NPFS) e é montado através do caminho especial \\. \\pipe\\ (isto é, um pipe nomeado como “foo” deve ter um nome de caminho de \\.\\pipe\\foo). O pipe anônimo dos comandos de pipeline é na realidade um pipe nomeado com um nome aleatório. Pipe nomeado numa rede WindowsO pipe nomeado também é utilizado como protocolo de rede da suíte Server Message Block (SMB). O IPC do SMB pode passar o contexto de autenticação do usuário de maneira transparente através de pipes nomeados. Toda a suíte de serviços de um domínio do Windows NT é implementada através de pipes nomeados. ConclusãoO Pipe é um mecanismo de extrema importância na comunicação entre processos, possibilitando que o processamento se dê de forma simples, flexível e eficiente, com baixo acoplamento e alta coesão. Referências BibliográficasFEITOSA, Samuel. “Pipes e Pipes Nomeados ou Fifos.” Blog de Samuel Feitosa, Jul 2009. http://samuca.wordpress.com/2007/05/02/pipes-e-pipes-nomeados-ou-fifos/. Acesso em: 5 nov 2009 PIPE nomeado. In: Wikipédia: a enciclopédia livre. Disponível em: http://pt.wikipedia.org/wiki/Pipe_nomeado Acesso em: 5 nov 2009 RIBEIRO, Uirá. Certificação Linux. 1° ed. Axcel Books, 2004.","link":"/2010/01/15/pipes-comunicacao-entre-processos/"},{"title":"Introdução ao Firewall IPCop","text":"Hoje iremos introduzir o Firewall IPCop, um sistema de segurança muito eficiente e extremamente funcional. O IPCop é uma distribuição Linux que possui como principal foco a proteção de redes de pequeno a médio porte, fazendo isso de maneira facilitada através de uma interface web muito intuitiva e funcional. Este é derivado de um projeto de nome Smoothwall e carrega até hoje, apesar de serem projetos desenvolvidos independentemente, grandes similaridades. Essa similaridade certamente é determinante para possibilitar que a grande maioria de Add-ons disponibilizados para IPCop também se encontrem disponíveis para o Smoothwall. A interface do mesmo, como mencionado anteriormente, é muito intuitiva, o que possibilita que qualquer pessoa com um mínimo de conhecimento em redes possa administrar as regras do Firewall. Abaixo demonstramos uma imagem da interface WEB desse sistema, para que possam ter uma idéia de como ela é: Figura1: Imagem da tela de configuração do servidor DHCP Outros Screenshots podem ser vistos aqui ! Inicialmente é importante deixar claro que este é um projeto com grandes virtudes, porém com algumas falhas também, porém essas falhas não são realmente significativas para um uso “normal” da ferramenta, e podem ser contornadas facilmente. O IPCop já trás em sua instalação padrão uma serie de funcionalidades, como por exemplo, Servidor Proxy, DHCP, NTP, DNS, SSH, Ferramentas de Conexão PPPoe e VPN, além de diversos relatórios gerenciais, no entanto ainda podemos adicionar diversas outras funcionalidades nos fazendo uso de Add-ons aos quais normalmente são fornecidos por terceiros. No momento que escrevo este artigo, o IPCop está na versão estável 1.4.20, sendo que a mesma já possui disponível um patch de segurança ao qual poderá ser instalado assim que finalizada a instalação, esta atualização elevará o mesmo para a versão 1.4.21. A atualização no IPCop é muito facilitada bastando entrar no menu: Sistemas &gt; Atualizações e clicar no botão “Atualizar lista de atualizações”, assim caso sua conexão com a internet estiver funcionando, o mesmo ira encontrar a atualização mencionada, bastando para finalizar apenas clicar no botão instalar. O IPCop possui um versão beta do que promete ser sua nova versão, a 1.9.11. Além desta versão beta, existe um projeto de uma segunda geração do software, a qual promete muitas novidades, tanto de interface quanto de funcionalidades, para mais informações podem acessar aqui, a página esta em inglês mais a leitura é bem tranquila e podemos perceber algumas das principais mudanças que ocorrerão. Além da mudança da interface que é evidente, destacamos como principal novidade a implementação do kernel 2.6, visto que o mesmo até sua ultima versão estável trabalha sobre o kernel 2.4. Com o IPCop instalado podemos notar muitas funcionalidades, como por exemplo, Um sistema de agendamento de desligamento o que em determinados casos pode ser bem útil. Com esse sistema você pode definir que horas vai acontecer a ocorrência, quais dias da semana e qual ação, no caso desligar ou reiniciar a maquina. Possui também um sistema de backup de suas configurações, possibilitando voltar a uma configuração anterior sem muito desgaste. Várias outras ferramentas estão disponíveis, essas são algumas apenas. Add-onsOs Add-ons são disponibilizados por projetos parelelos ao IPCop e varios deles possuem funcionalidades cruciais para melhorar a funcionalidade do software, essa importância é reconhecida até pelos envolvidos no projeto do mesmo, visto que a versão 2.0 do IPCop provavelmente já vira com varios deles em seu pacote base. Abaixo destacaremos alguns desses Add-ons citando suas funções, bem como onde podem ser encontrado informações adicionais. Advanced Web Proxy: Adiciona uma série de funcionalidades ao servidor Proxy já previamente instalado com o IPCop. Mais informações: Site do Desenvolvedor, Screenshots , Download , Guia do Administrador URL filter: Responsável por adicionar um maior controle no tráfego de sua rede, como esse Add-on é possível bloquear sites por categoria, por domínio, URL ou arquivo, com ele é possível também integrar a sistemas de terceiros para atualizar suas listas de categorias, um exemplo bom é o do sistema Shalla Secure Services que possui atualização muito freqüente e é muito bem estruturada, e é possível fazer com que o próprio IPCop busque estas atualizações automaticamente. Mais informações: Site do Desenvolvedor, Screenshots, Download, Documentação BlockOutTraffic (BOT): Este é um Add-on muito interessante, e certamente faz a diferença! Este adiciona funcionalidades como política de segurança padrão para o trafego de saída, assim você pode definir quais máquinas tem ou não permissão para executar a tarefa solicitada. Com esse podemos criar um sistema de controle muito mais efetivo. Mais informações: Site do Desenvolvedor, Screenshots, Download, Documentação Zerina (OpenVPN): Add-on que acrescenta uma opção a solução de VPN já previamente instalada com o IPCop, este possui configuração muito intuitiva, além de possuir clientes para diversas plataformas, inclusive o Windows. Mais informações: Site do Desenvolvedor, Download, Excelente guia em Português Abaixo relacionamos outros Add-ons bem comentados: CopFilter - SiteCalamaris Proxy Report Generator - SiteNMAP - Site Outros Add-ons podem ser encontrados aqui! O intuito deste foi apenas uma introdução ao IPCop, porém brevemente estaremos postando aqui mais artigos relacionados que cubrirão desde detalhes da instalação até algumas configurações mais avançadas no mesmo, caso tenha ficado com alguma duvida sobre o artigo sintam-se à vontade para deixar seu comentário!","link":"/2010/01/15/introducao-ao-firewall-ipcop/"},{"title":"Segurança da Informação","text":"O mercado de segurança em TI vem crescendo a cada ano e se tornando um dos pilares de sustentação de empresas que investiram montantes consideráveis em toda a infra- estrutura de TI. Toda essa infra-estrutura, cada vez mais complexa e interligada, com múltiplos pontos de acesso, demanda a adoção de soluções de segurança capazes de monitorar as tentativas de violação dos dados gerados por inúmeras transações. Assim surgiu à necessidade de se utilizar melhores mecanismos para prover a segurança das transações de informações confidenciais. A questão segurança é bastante enfatizada, principalmente, quando se imagina, a possibilidade de se ter suas informações, expostas à atacantes ou intrusos da Internet, que surgem com meios cada vez mais sofisticados para violar a privacidade e a segurança das comunicações. Devido a estas preocupações, a proteção da informação tem se tornado um dos interesses primários dos administradores de sistemas. A Segurança da Informação consiste na certeza de que as informações de uso restrito não devem ser acessadas, copiadas ou si quer lidas por pessoas não autorizadas. A informação pode existir de diversas formas. Pode ser, impressa, escrita, armazenada de forma eletrônica ou transmitida via e-mail. Independente da forma que é apresentada ou meio pelo qual a informação é compartilhada ou armazenada, é recomendado que ela seja sempre protegida de forma adequada. Durante este trabalho estarei dando maior ênfase a parte da segurança ligada a área de computação, já que a segurança é um tema muito amplo e falar nela como um todo poderia deixar meu trabalho muito superficial. GLOSÁRIOAtaque: Evento que pode comprometer a segurança de um sistema ou uma rede. Um ataque pode ter ou não sucesso. Um ataque com sucesso caracteriza uma invasão. Autenticação: É o processo de se confirmar a identidade de um usuário ou um host, esta pode ser feita na camada de aplicação (através de uma senha), ou mais complexa, utilizando algoritmos específicos. Bug: Uma falha, ou fraqueza em um sistema de computador. Cavalo de Tróia: Uma aplicação ou código que, sem o conhecimento do usuário realiza alguma tarefa que compromete a segurança de um sistema, em geral, esta aplicação se apresentação usuário de forma rotineira e legítima. Denial of Service: Interrupção de serviço. Engenharia Social: Técnica utilizada por hackers para obter informações interagindo diretamente com as pessoas. Exploits: Programa ou parte de um programa malicioso projetado para explorar um vulnerabilidade existente em um software de computador. Firewall: Equipamento e/ou software utilizado para controlar as conexões que entram ou saem de uma rede. Eles podem simplesmente filtrar os pacotes baseados em regras simples, como também fornecer outras funções tais como: NAT, proxy, etc. HTTP: Do inglês HyperText Transfer Protocol. Protocolo usado para transferir páginas Web entre um servidor e um cliente. Invasão: Caracteriza um ataque bem sucedido. NAT: Network Address Translation - Mecanismo que permite a conexão de redes privadas à rede Internet sem alteração dos endereços reservados. Através de um NAT server os endereços de rede reservados são convertidos para endereços públicos quando se torna necessário o acesso à rede Internet. Com este mecanismo, diversos computadores com endereços internos podem compartilhar um único endereço IP. Scanner: Ferramenta utilizada por hackers ou especialistas em segurança que serve para “varrer” uma máquina ou uma rede, em busca de portas abertas, informações ou serviços vulneráveis. SMTP: Do inglês Simple Mail Transfer Protocol. Protocolo padrão para envio de e-mail através da Internet. Stealth: São os programas que tem habilidade de agir sem ser detectado. Trojan (Cavalo de Tróia): Uma aplicação ou código que, sem o conhecimento do usuário realiza alguma tarefa que compromete a segurança de um sistema, em geral, esta aplicação se apresenta ao usuário de forma rotineira e legítima. Vírus: São códigos ou programas que infectam outros programas e se multiplicam, na maioria das vezes podem causar danos aos sistemas infectados. Vulnerabilidade: Estado de um componente de um sistema que compromete a segurança de todo o sistema, uma vulnerabilidade existe sempre, até que seja corrigida, existem vulnerabilidades que são intrínsecas ao sistema. Um ataque explora uma vulnerabilidade. Worm: Um worm é semelhante a um vírus, mas difere pelo fato de não necessitar de um programa específico para se infectar e reproduzir. Muitos vírus hoje, possuem a característica de worms e vice e versa. CONCEITOS BÁSICOS DA SEGURANÇA DA INFORMAÇÃOA segurança da informação é essencial para qual quer empresa seja ela de grande ou pequeno porte, pois as vulnerabilidades existem, os ataques também existem e crescem a cada dia, tanto em quantidade quanto em qualidade. Uma infra-estrutura de segurança não é só necessária como obrigatória, devendo existir, além de um investimento específico, um planejamento, uma gerência e uma metodologia bem definida. PILARES DE UM SISTEMA SEGUROOs pilares de um sistema seguro são: Integridade, Autenticação, Não-repúdio ou irrevogabilidade e Disponibilidade. Figura 1: Diagrama da Segurança da informação INTEGRIDADEDeve ser possível ao receptor de uma mensagem verificar se esta foi alterada durante o trânsito. AUTENTICAÇÃODeve ser possível ao receptor de uma mensagem, verificar corretamente sua origem, um intruso não pode se fazer passar (personificar) o remetente desta mensagem. NÃO-REPÚDIO OU IRREVOGABILIDADEO remetente de uma mensagem não deve ser capaz de negar que enviou a mensagem. DISPONIBILIDADESe refere ao sistema estar sempre pronto a responder requisições de usuários legítimos. Para conseguirmos isto existem diversos artifícios como por exemplo: NO-BREAKS Figura 2: Exemplo de um No-break gerenciável Os problemas de energia elétrica são as maiores causas de defeitos nos sistemas de computadores e na perda de dados. O uso de sistemas de controle de energia tipo No-Break proporcionará a proteção necessária e evitará problemas nos sistemas de computadores, equipamentos eletrônicos e de telecomunicações, centrais telefônicas, etc. A grande vantagem de usar um no-break é ter garantia de um fornecimento contínuo de eletricidade. Mesmo que ocorra um pique de energia ou o fornecimento seja cortado, você poderá continuar trabalhando até que as baterias do no-break se esgotem, tendo tempo para salvar seus documentos e desligar tranqüilamente o todo o sistema. Existem dois tipos de no-breaks, os on-line e os off-line. Os primeiros, os on-line, são melhores, pois neles a bateria é alimentada continuamente e o micro é alimentado diretamente pela bateria, tendo um fornecimento 100% estável. Nos no-breaks off-line a energia da toma é repassada diretamente para o micro, sendo a bateria usada apenas quando a corrente é cortada, não oferecendo uma proteção tão completa quanto o primeiro. Em geral o no-break sinaliza de forma visual e sonora logo quando a energia é cortada. Conforme a bateria for ficando fraca a sinalização vai se tornando cada vez mais frequente. A autonomia das baterias varia de acordo com a capacidade das baterias (medida em VAs) e o consumo elétrico do sistema ligado ao mesmo. A autonomia ideal para um sistema é de ao menos 15 minutos, o que em geral será suficiente para terminar algo mais urgente e salvar tudo antes de desligá-lo. Muitos no-breaks vêm com a possibilidade de gerenciamento, através de interfaces inteligentes. Nestes casos, ligando o no-break a uma das saídas do micro (normalmente serial) e instalando o software que o acompanha, você poderá programar o no-break para que salve os documentos e desligue o micro automaticamente no caso de corte de energia. SISTEMAS REDUNDANTESHOT-SWAP Figura 3: Modulo com tecnologia Hot-swap Hot-swap é uma tecnologia muito usada em servidores de rede. Ela permite a troca de dispositivos SCSI com o micro ligado, como, por exemplo, discos rígidos. Se o disco rígido do servidor queima, o técnico pode substituir o disco sem a necessidade de desligar e abrir o micro. RAIDRAID (Redundant Arrays of Independent Disks - Matrizes Redundantes de Discos Independentes) é uma tecnologia consagrada que oferece capacidade, confiabilidade, alto desempenho e economia no armazenamento de dados on-line. Muito superior a discos magnéticos, o sistema RAID é de ampla utilização em todo o espectro da indústria de computação, desde o PC até o mainframe. O sistema RAID gerencia um conjunto de discos, mas aparece ao usuário como um único disco grande. A vantagem dos discos múltiplos é que, em caso de falha, os dados são transferidos para um disco próximo e o sistema continua trabalhando, sem perda de dados. A disponibilidade dos dados também é mais rápida. Os múltiplos discos de um sistema RAID podem ser varridos simultaneamente. A procura em um único disco grande demoraria muito mais. A transferência de dados de RAID para RAID também é mais rápida, porque os discos podem ser acessados simultaneamente. A manutenção também é mais fácil com o RAID, e a tolerância de falhas, mais alta. Cada disco pode ser substituído enquanto o sistema trabalha. Com essa capacidade de “hot-swap”, os administradores de rede podem economizar tempo e evitar possíveis problemas antes que eles coloquem a operação do sistema em perigo. Há vários níveis ou tipos de RAID para acomodar necessidades diferentes de armazenamento: RAID 1 - Este nível tem discos duplicados trabalhando lado a lado, em “espelhamento de discos” paralelo. A confiabilidade do sistema é muito mais alta. Se um disco falhar, o outro pode fornecer quaisquer dados necessários. Entretanto, apenas 50% da capacidade do drive está disponível para armazenamento. RAID 2 - Não é usado por não ser compatível com os drives atuais. RAID 3 - Este nível usa o striping de dados e um drive de paridade dedicado. Quando os dados são escritos na matriz, um byte vai para cada disco. Cada drive é acessado ao mesmo tempo. A vantagem é uma transferência de dados muito mais alta. A desvantagem é que, como cada drive é usado, apenas uma transação de I/O pode ser processada de cada vez. O RAID 3 é o melhor para grandes requisições de dados. RAID 4 (RAID 0) - Neste nível, os blocos de dados são divididos ao longo da matriz de discos e, portanto, os discos podem ser acessados em paralelo. O RAID 4 tem uma taxa de I/O maior que o RAID 3, mas a transferência de dados é mais lenta. Os drivers de paridade podem ser utilizados para dar tolerência a falhas do drive de dados. O RAID 4 sem paridade é conhecido como RAID 0. RAID 5 - De modo diferente do RAID 3, que acessa todos os drivers ao mesmo tempo para a mesma leitura ou escrita, o RAID 5 pode acessar tantos drives quanto possível para leituras e escritas diferentes. Como resultado, oferece a maior taxa de I/O de todos os níveis de RAID. FONTE COM REDUNDÂNCIA Figura 4: Exemplo de fonte redundante Este equipamento consiste em 2 módulos de potência projetados para trabalhar de modo a que se um dos módulos falhar o outro continue, garantindo que o computador mantenha o funcionamento mesmo em caso de falha de um dos módulos da fonte. POLÍTICA DE SEGURANÇA DA INFORMAÇÃOA Política de Segurança é a formalização de todos os aspectos considerados relevantes por uma organização para proteção, controle e monitoramento de seus recursos computacionais. Também deve ser vista como um canal de comunicação entre usuários e o comitê Corporativo de Segurança da Informação. A documentação gerada precisa explicar a importância da segurança para motivar as pessoas envolvidas a praticá-la. A divulgação da política de segurança é uma das ferramentas responsáveis pelo sucesso da sua implantação. Seu objetivo é disseminar a política de segurança da informação na empresa, conscientizando os colaboradores e prestadores de serviço para a política de segurança que está sendo implantada. Deverão ser desenvolvidas palestras de conscientização, cartas, e-mails, cartilhas e eventos objetivando o sucesso da implantação. A política de segurança não define procedimentos específicos de manipulação e proteção da informação, mas atribui direitos e responsabilidades às pessoas (usuários, administradores de redes e sistemas, funcionários, gerentes, etc.) que lidam com essa informação. Desta forma, elas sabem quais as expectativas que podem ter e quais são as suas atribuições em relação à segurança dos recursos computacionais com os quais trabalham. Além disso, a política de segurança também estipula as penalidades às quais estão sujeitos àqueles que a descumprem. Antes que a política de segurança seja escrita, é necessário definir a informação a ser protegida. Usualmente, isso é feito através de uma análise de riscos, que identifica: Recursos protegidos pela política; Ameaças às quais estes recursos estão sujeitos; Vulnerabilidades que podem viabilizar a concretização destas ameaças, analisando-as individualmente. Uma política de segurança deve cobrir os seguintes aspectos: Política de senhas; Direitos e responsabilidades dos usuários; Direitos e responsabilidades do provedor dos recursos; Ações previstas em caso de violação da política. INSTALAÇÃO DE UM SISTEMA SEGUROINSTALAÇÃOUm sistema mais seguro começa pela instalação do mínimo possível de pacotes e componentes, especialmente os que implementam serviços de rede. Este mínimo depende fundamentalmente do propósito do sistema em questão e do ambiente de rede no qual ele está inserido. Por exemplo, em princípio um sistema dedicado a servir páginas Web não precisa de um software servidor SMTP, assim como uma estação de trabalho não precisa de um servidor HTTP . A justificativa para esta recomendação é bastante simples. É comum que serviços não utilizados não sejam monitorados por falhas de segurança, o que aumenta a possibilidade de não ser aplicada uma correção necessária. A redução no número de pacotes instalados diminui a chance de que o sistema possua uma vulnerabilidade que possa vir a ser explorada por um atacante. DESATIVAÇÃO DE SERVIÇOS NÃO UTILIZADOSApós a instalação a primeira precaução a ser tomada deve ser a verificação se tudo o que está instalado na maquina é realmente necessário, existem programas aos quais por padrão já ativam alguns serviços aos quais não fazemos uso, e cabe ao administrador a localização destes serviços e a desativação e se possível a até remoção dos mesmos. Figura 5: Exemplo da tela de serviços carregados no Windows 2000 Server INSTALAÇÃO DE CORREÇÕESDepois de um sistema ter sido corretamente instalado e configurado, é necessário verificar se não existem correções (patches, fixes, service packs) para vulnerabilidades conhecidas nos componentes instalados. A maioria dos fornecedores de software libera correções para problemas de segurança que sejam descobertos em um sistema, sem que se tenha de esperar pela sua próxima versão. Na maioria das vezes, estas correções estão disponíveis através da Internet. A instalação de correções deve ser realizada não só como parte da instalação inicial do sistema, mas também durante o seu tempo de vida, a intervalos periódicos ou sempre que surgirem vulnerabilidades que o afetem. No Windows temos a opção chama Windows Update, ferramenta a qual busca automaticamente todas as atualizações as quais ainda não foram instaladas. Figura 6: Windows Update em funcionamento No Linux temos uma ferramenta chamada APT (Advanced Package Tool) com esta ferramenta você pode instalar, remover, reconfigurar e também atualizar pacotes. Num entanto o APT ao contrario do windows update que é praticamente todo automático, deve ser configurado para saber de onde pegar os pacotes. Para executar esta configuração devemos executar o comando apt-setup ou podemos editar manualmente o arquivo /etc/apt/sources.list. Figura 7: APT em funcionamento GERAÇÃO DE LOGSLogs são muito importantes para a administração segura de sistemas, pois registram informações sobre o seu funcionamento e sobre eventos por eles detectados. Muitas vezes, o log é o único recurso que um administrador possui para descobrir as causas de um problema ou comportamento fora do esperado pelo sistema. Figura 8: Exemplo de Log obtido em servidor de internet AMEAÇAS A QUE ESTAMOS EXPOSTOSPara conseguirmos nos defender das ameaças às quais estamos expostos, primeiramente devemos conhecê-las, assim teremos mais chances de conseguirmos obter melhores resultados nesta luta por maior segurança. As principais ameaças a que estamos expostos seriam os vírus, trojans, worms, hackers. VÍRUSCARACTERÍSTICASO que comumente chamamos de “vírus de computador” são programas que possuem algumas características em comum com os vírus biológicos: São pequenos; Um vírus, por definição, não funciona por si só. Deve infectar um arquivo executável ou arquivos que utilizam macros, ou seja, em geral o vírus fica escondido dentro da série de comandos de um programa maior; Contém instruções para parasitar e criar cópias de si mesmo de forma autônoma e sem autorização específica (e, em geral, sem o conhecimento) do usuário para isso - eles são, portanto, auto replicantes. A INFECÇÃOHá várias manifestações visíveis da atividade dos vírus: mostrar mensagens, alterar ou deletar determinados tipos de arquivos, corromper a tabela de alocação, diminuir a performance do sistema ou até formatar o disco rígido. Muitas vezes a ação de um vírus só se inicia a partir de eventos ou condições que seu criador pré-estipulou: atingir certa data, um número de vezes que um programa é rodado, um comando específico ser executado, etc. Um vírus pode atingir um computador a partir de diferentes “vetores” todos previamente infectados: documentos, programas, disquetes, arquivos de sistema, etc. Arquivos executáveis ( _.exe. _.bat, _.com) são particularmente perigosos e deve-se evitar enviá-los ou recebê-los. Após infectar o computador, eles podem passar a atacar outros arquivos. Se um destes arquivos infectados for transferido para outro computador, o vírus vai junto e, quando for executado irá contaminar a segundo máquina. Arquivos de dados, som ( _.wav, .mid), imagem (.bmp, .pcx, _.gif, _.jpg), vídeo (.avi, _.mov) e os de texto que não contenham macros ( _.txt, _.wri) podem ser abertos sem problemas. Mas, tanto o download (cópia de programas, via http ou ftp) como o serviço de correio eletrônico (e-mail), possibilitam a entrada de arquivos no computador. Assim, a internet tornou-se um grande foco de disseminação de vírus, worms, trojans e outros programas maliciosos, por facilitar em muito o envio e recepção de arquivos (o que antes era feito basicamente por meio de disquetes). Como um dos mais populares serviços da Internet é o correio eletrônico, o envio de programas invasores por e-mail é preocupante. Como regra geral pode-se assumir que não devemos executar arquivos recebidos, especialmente os arquivos executáveis, mesmo que se conheça o remetente e que se tenha certeza que ele é cuidadoso e usa antivírus atualizado. Mas, na quase totalidade dos casos pode-se admitir que a simples recepção e a visualização de uma mensagem não contaminam o computador receptor. Figura 9: Exemplo de e-mail com link para um vírus TIPOS DE VÍRUSVÍRUS DE BOOT (MASTER BOOT RECORD / BOOT SECTOR VIRUSES)Todos os discos e disquetes possuem uma área de inicialização reservada para informações relacionadas à formatação do disco, dos diretórios e dos arquivos nele armazenados (registro mestre do Sistema, o Master Boot Record - MBR dos discos rígidos ou a área de boot dos disquetes - Boot Sector). Como essa área é executada antes de qualquer outro programa (incluindo qualquer programa Antivírus), esses vírus são muito bem sucedidos. Para esse sucesso também contribui o fato da infecção poder ocorrer por meio de um ato simples do usuário: esquecer um disquete contaminado dentro do drive A. Como todos os discos possuem também um pequeno programa de boot (que determina onde está ou não o sistema operacional e reconhece, inclusive, os periféricos instalados no computador), os vírus de boot podem se “esconder” em qualquer disco ou disquete. A contaminação ocorre quando um boot é feito através de um disquete contaminado. O setor de boot do disquete possui o código para determinar se um disquete é “bootável” ou para mostrar a mensagem: “Disquete Sem Sistema ou Erro de Disco”. É este código, gravado no setor de boot que, ao ser contaminado, assume o controle do micro. Assim que o vírus é executado ele toma conta da memória do micro e infecciona o MBR do disco rígido. A disseminação é fácil: cada disquete não contaminado, ao ser colocado no drive e ser lido pode passar a ter uma cópia do código e, nesse caso, é contaminado e passa a ser um “vetor”. VÍRUS DE PROGRAMA (FILE INFECTING VIRUSES)Os vírus de programa infectam - normalmente - os arquivos com extensão.exe e.com (alguns contaminam arquivos com outras extensões, como os .dll, as bibliotecas compartilhadas e os .ovl). Alguns deles se replicam, contaminando outros arquivos, de maneira silenciosa, sem interferir com a execução dos programas que estão contaminados. Assim sendo, pode não haver sinais perceptíveis do que está acontecendo no micro. Alguns dos vírus de Programa vão se reproduzindo até que uma determinada data, ou conjunto de fatores, seja alcançado. Somente aí é que começa a sua ação. A infecção se dá pela execução de um arquivo já infectado no computador. Há diversas origens possíveis para o arquivo infectado: Internet, Rede Local ou um disquete. VÍRUS MULTIPARTITEÉ uma mistura dos tipos de boot e de programa, podendo infectar ambos: arquivos de programas e setores de boot. São mais eficazes na tarefa de se espalhar, contaminando outros arquivos e/ou discos e são mais difíceis de serem detectados e removidos. VÍRUS DE MACROQuando se usa alguns programas, por exemplo um editor de texto, e necessita-se executar uma tarefa repetidas vezes em seqüência (por exemplo, substituir todos os “vc” por “você”) pode-se editar um comando único para efetuá-las. Esse comando é chamado de macro, que pode ser salvo em um modelo para ser aplicado em outros arquivos. Além dessa opção da própria pessoa fazer um modelo os comandos básicos dos editores de texto também funcionam com modelos. Os vírus de macro atacam justamente esses arquivos comprometendo o funcionamento do programa. Os alvos principais são os próprios editores de texto (Word) e as planilhas de cálculo (Excel). A disseminação desse tipo de vírus é muito mais acentuada pois documentos são muito móveis e passam de máquina em máquina (entre colegas de trabalho, estudantes, amigos e outras pessoas). Ao escrever, editar ou, simplesmente, ler arquivos vindos de computadores infectados a contaminação ocorre. Assim, verdadeiras “epidemias” podem acontecer em pouco tempo. Além disso, os macrovírus constituem a primeira categoria de vírus multiplataforma, ou seja, não se limitam aos computadores pessoais, podendo infectar também outras plataformas que usem o mesmo programa, como o Macintosh, por exemplo. Um outro agravante em relação a esses vírus é a facilidade de lidar com as linguagens de macro, dispensando que o criador seja um especialista em programação. Isso acarretou no desenvolvimento de muitos vírus e inúmeras variantes e vírus de macro, num período curto de tempo. OUTRAS CAPACIDADESPara tentar impedir a detecção pelos antivírus algumas capacidades foram dadas a qualquer um dos tipos de vírus acima. Assim, cada um desses três tipos de vírus podem ter outras características, podendo ser: POLIMORFISMOTêm como principal característica o fato de estar sempre em mutação, ou seja, esse vírus muda ao criar cópias dele mesmo, alterando seu código. Mas, os clones são tão funcionais quanto seu original, ou mais. O objetivo da mudança é tentar dificultar a ação dos antivírus, criando uma mutação, algo diferente daquilo que a vacina procura. INVISIBILIDADETêm a capacidade de, entre outras coisas, temporariamente se auto remover da memória, para escapar da ação dos programas antivírus. ENCRIPTAÇÃONesses é muito difícil a ação da vacina. Assim mesmo que seja detectado o antivírus vai ter grande problema para removê-lo. TROJAN (CAVALOS DE TRÓIA)O vírus do tipo Trojan tipicamente se disfarça como algo desejável — por exemplo, um programa legítimo. Assim como seu equivalente histórico, porém, ele guarda um poder de ataque oculto. O Trojan geralmente não se replica (embora pesquisadores tenham descoberto Trojans replicantes). Ele espera até que aconteça o evento de gatilho e então mostra uma mensagem ou destrói arquivos ou discos. Como ele geralmente não se replica, alguns pesquisadores não classificam os Trojan Horses como vírus. Figura 10: Exemplo de Trojan WORMS (VERMES)O worm é um programa projetado para se copiar rapidamente de um computador para outro, através de alguma mídia de rede: e-mail, TCP/IP, etc. De acordo com Cary Nachenburg, Pesquisador Chefe no Symantec AntiVirus Research Center (SARC, Centro de Pesquisas Antivírus), “Worms são insidiosos porque eles pouco dependem (ou não dependem) do comportamento humano para se espalhar de um computador para outro.” A maioria dos vírus, dependem de algum tipo de gatilho do usuário, como abrir um anexo, reinicializar uma máquina, ou executar um programa. Worms, no entanto, são capazes de funcionar de forma mais independente. Um exemplo disto é o vírus Explore.zip, que pode identificar programas de e-mail amplamente utilizados, como o MS Outlook, que possam existir num computador, e sistematicamente começar a enviar cópias de si mesmo para todos na lista de e-mail do usuário. Além disso, o worm está mais interessado em infectar quantas máquinas forem possíveis na rede, e menos interessado em espalhar muitas cópias de si mesmo em computadores individuais (como os primeiros vírus de computador). Os worms geralmente são classificados em worms de e-mail ou worms de protocolo, dependendo do vetor primário pelo qual eles se espalham. Ambos os tipos podem ser transferidos, com conhecimento ou não, através da web. HACKERSO perfil típico do hacker é: jovem entre 15 e 25 anos, com amplo conhecimento de redes, conhecimento de programação (geralmente em linguagens como C, C++, Java e Assembler). Contudo, existem diversos tipos de “hackers”, dos que possuem mais experiência para os que apenas “copiam” furos de segurança explorados por outros hackers. São eles: WHITE-HATS: Os white-hats são os hackers que exploram problemas de segurança para divulgá-los abertamente, de forma que toda a comunidade tenha acesso à informações sobre como se proteger. Desejam abolir a “segurança por obscuridade”, que nada mais é do que tentar proteger ou manter a segurança pelo segredo de informações sobre o funcionamento de uma rede, sistema operacional ou programa em geral. Seu lema é o “full disclosure”, ou conhecimento aberto, acessível a todos. BLACK-HATS: Ao contrário dos white-hats, apesar de movidos também pela curiosidade, usam suas descobertas e habilidades em favor próprio, em esquemas de extorsão, chantagem de algum tipo, ou qualquer esquema que venha a trazer algum benefício, geralmente, e obviamente, ilícito. Estes são extremamente perigosos e difíceis de identificar, pois nunca tentarão chamar a atenção. Agem da forma mais furtiva possível. CRACKERS: As denominações para os crackers são muitas. Alguns classificam de crackers, aqueles que têm por objetivo invadir sistemas em rede ou computadores apenas pelo desafio. Contudo, historicamente, o nome “cracker” tem uma relação com a modificação de código, para obter funcionalidades que não existem, ou de certa forma, limitadas. Um exemplo clássico são os diversos grupos existentes na Internet que tem por finalidade criar “patches” ou mesmo “cracks” que modificam programas comerciais (limitados por mecanismos de tempo por exemplo, como shareware), permitindo seu uso irrestrito, sem limitação alguma. PHREAKERS: Apesar de muitos considerarem um cientista russo chamado Nicola Tesla (que na virada do século realizava experiências assustadoras – até para os dias de hoje – com eletricidade) como o primeiro hacker da história, os primeiros hackers da era digital lidavam com telefonia. Sua especialidade é interferir com o curso normal de funcionamento das centrais telefônicas, mudar rotas, números, realizar chamadas sem tarifação, bem como realizar chamadas sem ser detectado (origem). Com a informatização das centrais telefônicas, ficou inclusive mais fácil e acessível o comprometimento de tais informações. Kevin Mitnick, considerado o maior hacker de todos os tempos, era um ótimo phreaker. Na fase final de sua captura, quando os agentes de governo ajudados pelo Tsutomu Shimomura estavam chegando a um nome, ele conseguia enganar as investigações através do controle que tinha da rede de telefonia da GTE (concessionária telefônica dos EUA). WANNABES: Os wannabes ou script-kiddies são aqueles que acham que sabem, dizem para todos que sabem, se anunciam, ou divulgam abertamente suas “façanhas”, e usam em 99% dos casos scripts ou exploits conhecidos, já divulgados, denominados “receitas de bolo”, facilmente encontradas em sites como “www.rootshell.com”, ou “xforce.iss.net”. Estes possuem relação direta com a maioria dos usuários da Internet Brasileira. São facilmente encontrados em fórums de discussão sobre o tema, e principalmente no IRC. A maioria não possui escrúpulo algum, portanto, tomar medidas de cautela é aconselhável. Os wannabes geralmente atacam sem uma razão ou objetivo, apenas para testar ou treinar suas descobertas, o que nos torna, usuários Internet, potenciais salvos. ALGUNS MÉTODOS DE ATAQUE DOS HACKERSENGENHARIA SOCIAL Existe algum método mais rápido e eficiente de se descobrir uma senha? Que tal simplesmente perguntar? Por mais extraordinário que possa parecer, o método mais simples, mais usado e talvez mais eficiente de se recolher informações é simplesmente chegar e perguntar. Você também poderia subornar, mas dependendo da situação, isto pode lhe custar muito caro, então por que não tentar enganar e obter tais informações? De fato, este método é bastante utilizado, e existem hackers que sabem usá-lo com grande destreza. Essa tática de ataque é conhecida como “Engenharia Social”. Basicamente, esta é a arte de fazer com que outras pessoas concordem com você e atendam aos seus pedidos ou desejos, mesmo que você não tenha autoridade para tal. Popularmente, pode-se dizer que engenharia social é simplesmente a arte de se contar uma mentira bastante convincente. Dentro da área de segurança podemos definir engenharia social como a aquisição de informações preciosas ou privilégios de acesso por “alguém de fora”, baseado em uma relação de confiança estabelecida, inapropriadamente, com “alguém de dentro”. Profissionais utilizam este tipo de aproximação para adquirir informações confidenciais, como organogramas de organizações, números de cartões de crédito e telefone, senhas de acesso, diagrama da rede, etc. com o objetivo de avaliar as vulnerabilidades de uma organização para futuros ataques. Dizem que o único computador totalmente seguro é aquele desligado da tomada. A arte da engenharia social concentra-se no elo mais fraco da corrente da segurança de computadores: os seres humanos. O simples fato de que se pode facilmente convencer uma pessoa a ligar o computador, torna vulnerável, até mesmo, os computadores desligados. Na medida em que a parte humana de um sistema de segurança é a mais essencial, não existe computador na face da Terra que não necessite de seres humanos. Isso significa que essa é uma fraqueza universal, independente de plataforma, software, tipo de conexão de rede ou idade do equipamento. Qualquer pessoa com acesso a qualquer parte do sistema, física ou remota, pode ser uma falha de segurança em potencial. Qualquer informação adquirida pode ser utilizada para um outro ataque de engenharia social. Isso significa que qualquer pessoa, mesmo que não seja considerada integrante da política de segurança pode servir como uma porta de entrada. Como um ataque de engenharia social pode revelar muitas informações, como se pode tornar um sistema de computadores mais seguro? A resposta é educação e difusão da informação, explicando aos empregados e pessoas ligadas direta ou indiretamente ao sistema a importância de uma política de segurança, evitando assim o ataque de pessoas que poderão tentar manipulá-los para ganhar acesso a informações privadas. Isto já é um excelente começo para tornar segura sua rede ou sistema. DENIAL OF SERVICE (DOS) Os ataques DoS são bastante conhecidos no âmbito da comunidade de segurança de redes. Estes ataques, através do envio indiscriminado de requisições a um computador alvo, visam causar a indisponibilidade dos serviços oferecidos por ele. Fazendo uma analogia simples, é o que ocorre com as companhias de telefone nas noites de natal e ano novo, quando milhares de pessoas decidem, simultaneamente, cumprimentar à meia-noite parentes e amigos no Brasil e no exterior. Nos cinco minutos posteriores à virada do ano, muito provavelmente, você simplesmente não conseguirá completar a sua ligação, pois as linhas telefônicas estarão saturadas. Ao longo dos últimos anos, uma categoria de ataques de rede tem-se tornado bastante conhecida: a intrusão distribuída. Neste novo enfoque, os ataques não são baseados no uso de um único computador para iniciar um ataque, no lugar são utilizados centenas ou até milhares de computadores desprotegidos e ligados na Internet para lançar coordenadamente o ataque. A tecnologia distribuída não é completamente nova, no entanto, vem amadurecendo e se sofisticando de tal forma que até mesmo vândalos curiosos e sem muito conhecimento técnico podem causar danos sérios. Figura 11: Diagrama de um ataque DDos Atacante: Quem efetivamente coordena o ataque. Master: Máquina que recebe os parâmetros para o ataque e comanda os agentes. Agente: Máquina que efetivamente concretiza o ataque DoS contra uma ou mais vítimas, conforme for especificado pelo atacante. Vítima: Alvo do ataque. Máquina que é “inundada” por um volume enormede pacotes, ocasionando um extremo congestionamento da rede e resultando na paralização dos serviços oferecidos por ela. FERRAMENTAS DE AUXILIO A SEGURANÇAANTIVÍRUSA partir do surgimento dos primeiros vírus de computador e suas conseqüências, começaram a ser desenvolvido em todo mundo diferentes alternativas de prevenção contra estas infecções, recuperando então arquivos infectados ao seu estado original e minimizando os danos causados. Desta forma, começaram a surgir diferentes soluções antivírus com variadas tecnologias. Basicamente podemos concluir que Antivírus é um programa utilizado para descontaminar um computador ou rede que estiver infectado com vírus, worm e códigos maliciosos, bem como fornecer proteção contra novas invasões. MODO DE DETECÇÃO DOS VÍRUSUm vírus de computador é igual a qualquer outra aplicação, está composto de uma série de instruções e ao ser executado irá cumprir a ação para qual foi programado. Um conjunto de instruções que contém o código de malicioso permite sua identificação de maneira única através da forma como é programado. Estes parâmetros constituem o nome de um respectivo vírus. Algumas soluções antivírus conseguem identificar pequenas alterações no código malicioso original reconhecendo então um vírus modificado. Algumas empresas antivírus catalogam os vírus individualmente e outras por famílias de vírus, isso faz com que a informação disponibilizada por cada empresa sobre um determinado vírus varie constantemente. Quando uma solução antivírus detém a tecnologia de detectar um vírus a partir de seu código, pode ocorrer falsos alarmes de detecção, isto é denominado como falso positivo, mais este é um inconveniente necessário, pois esta analise permite detectar vírus desconhecidos quando executados. Quando uma solução antivírus detecta um vírus a partir de assinatura, é improvável a ocorrência de falsos alarmes ou falso positivo. Mais somente vírus analisados e catalogados são detectados ao serem executados, vírus desconhecidos não são detectados. PROCESSO DE ATUALIZAÇÃO DO ANTIVÍRUSO vírus de computador sempre é desenvolvido por um programador inescrupuloso (mal intencionado) que desenvolve esta aplicação maliciosa na maioria das vezes sem um objetivo claro. Com o surgimento da Internet e seus avançados meios de comunicação que permitem quase que simultaneamente o trafego de qualquer tipo de informação e aplicação, inclusive os vírus. Um vírus começa a disseminar através da Internet até que seja descoberto (geralmente a partir de seus efeitos), é então analisado pelas empresas de antivírus, são catalogados e então vacinas específicas serão são elaboradas. Todos estes processos são realizados em pouco tempo, é dificilmente ultrapassam horas. Estas vacinas (atualizações) ficam disponíveis nos servidores das respectivas empresas e seus usuários devem fazer o download dos arquivos necessários mantendo então o antivírus atualizado. HEURÍSTICAPouco tempo depois que surgiram as primeiras soluções antivírus, pode ser observado as limitações no desenvolvimento das analises de vírus através de métodos heurísticos, que se baseiam em complexos algoritmos matemáticos que tentam antecipar as ações que poderiam ocorrer quando um determinado código e executado. A tecnologia das Analises Heurística implementada por cada solução antivírus, nem sempre tem a mesma eficácia embora todas possui o mesmo objetivo, que é a detecção antecipada de vírus ainda desconhecidos. FIREWALLFirewall é um quesito de segurança com cada vez mais importância no mundo da computação. À medida que o uso de informações e sistemas é cada vez maior, a proteção destes requer a aplicação de ferramentas e conceitos de segurança eficientes. O firewall é uma opção praticamente imprescindível, podendo ser definido como uma barreira de proteção, que controla o tráfego de dados entre seu computador e a Internet (ou entre a rede onde seu computador está instalado e a Internet). Seu objetivo é permitir somente a transmissão e a recepção de dados autorizados. Existem firewalls baseados na combinação de hardware e software e firewalls baseados somente em software. Explicando de maneira mais precisa, o firewall é um mecanismo que atua como “defesa” de um computador ou de uma rede, controlando o acesso ao sistema por meio de regras e filtragem de dados. A vantagem do uso de firewalls em redes é que somente um computador pode atuar como firewall, não sendo necessário instalá-lo em cada máquina conectada. Figura 12: Ilustração da função de um Firewall Há mais de uma forma de funcionamento de um firewall, que varia de acordo com o sistema, aplicação ou do desenvolvedor do programa. No entanto, existem dois tipos básicos de conceitos de firewalls: o que é baseado em filtragem de pacotes e o que é baseado em controle de aplicações. FILTRAGEM DE PACOTESO firewall que trabalha na filtragem de pacotes é muito utilizado em redes pequenas ou de porte médio. Por meio de um conjunto de regras estabelecidas, esse tipo de firewall determina que endereços IPs e dados podem estabelecer comunicação e/ou transmitir/receber dados. Alguns sistemas ou serviços podem ser liberados completamente (por exemplo, o serviço de e-mail da rede), enquanto outros são bloqueados por padrão, por terem riscos elevados. O grande problema desse tipo de firewall, é que as regras aplicadas podem ser muito complexas e causar perda de desempenho da rede ou não serem eficazes o suficiente. Este tipo se restringe a trabalhar nas camadas TCP/IP, decidindo quais pacotes de dados podem passar e quais não. Tais escolhas são regras baseadas nas informações endereço IP remoto, endereço IP do destinatário, além da porta TCP usada. Quando devidamente configurado, esse tipo de firewall permite que somente computadores conhecidos troquem determinadas informações entre si e tenham acesso a determinados recursos. Um firewall assim, também é capaz de analisar informações sobre a conexão e notar alterações suspeitas, além de ter a capacidade de analisar o conteúdo dos pacotes, o que permite um controle ainda maior do que pode ou não ser acessível. FIREWALL DE APLICAÇÃOFirewalls de controle de aplicação (exemplos de aplicação: SMTP, FTP, HTTP, etc) são instalados geralmente em computadores servidores e são conhecidos como proxy. Este tipo não permite comunicação direto entre a rede e a Internet. Tudo deve passar pelo firewall, que atua como um intermediador. O proxy efetua a comunicação entre ambos os lados por meio da avaliação do número da sessão TCP dos pacotes. Este tipo de firewall é mais complexo, porém muito seguro, pois todas as aplicações precisam de um proxy. Caso não haja, a aplicação simplesmente não funciona. O firewall de aplicação permite um acompanhamento mais preciso do tráfego entre a rede e a Internet (ou entre a rede e outra rede). É possível, inclusive, contar com recursos de log e ferramentas de auditoria. Tais características deixam claro que este tipo de firewall é voltado a redes de porte médio ou grande e que sua configuração exige certa experiência no assunto. RAZÕES PARA UTILIZAR UM FIREWALLA seguir são citadas as 3 principais razões para se usar um firewall: 1 - O firewall pode ser usado para ajudar a impedir que sua rede ou seu computador seja acessado sem autorização. Assim, é possível evitar que informações sejam capturadas ou que sistemas tenham seu funcionamento prejudicado pela ação de hackers; 2 - O firewall é um grande aliado no combate a vírus e cavalos de tróia, uma vez que é capaz de bloquear portas que eventualmente sejam usadas pelas “pragas digitais” ou então bloquear acesso a programas não autorizados; 3 - Em redes corporativas, é possível evitar que os usuários acessem serviços ou sistemas indevidos, além de ter o controle sobre as ações realizadas na rede, sendo possível até mesmo descobrir quais usuários as efetuaram. BACKUPA palavra backup significa cópia de segurança, ou seja, quando fazemos backup estamos criando uma cópia dos arquivos importantes de forma que se ocorrer algum problema nos dados da empresa teremos uma cópia atualizada para restaurar todas as informações danificadas, evitando assim o desperdício de tempo e dinheiro para a recuperação de informações, o que nem sempre é possível, sua re-inclusão e eventual interrupção dos serviços prestados pela empresa por problemas técnicos. O Backup é a ferramenta de segurança mais importante de seu sistema. Sem o backup, dificilmente temos nosso trabalho recuperado por completo em caso de perda dos dados do disco rígido. É recomendado que o backup seja realizado, pelo menos, uma vez por semana e, em casos de informações importantes, uma vez por dia. O Winchester/HDD é um dos componentes mais sensíveis do computador, por isso, quando ocorrem quedas ou picos de energia elétrica ele pode ser danificado impedindo recuperação das informações. Quando um programa qualquer (ICQ ou antivírus, por exemplo) é instalado em um computador, ele pode modificar a estrutura de outros arquivos importantes (como arquivos do sistema operacional, por exemplo) de modo, que ele possa ser executado. Esta alteração pode comprometer o bom funcionamento do computador, danificar arquivos existentes ou provocar a perda de informações importantes. Existem três possibilidades de perda de dados, falhas técnicas, ambientais e humanas: Falhas técnicas: falha no subsistema de disco rígido (HD), falha de energia (resultam em dados corrompidos), sobrecarga na rede de computadores que podem gerar falhas de comunicação (resultam em dados corrompidos), falha de software nos sistemas. Falhas ambientais: descargas elétricas provindas de raios, enchentes. Falhas humanas: detém 84% das perdas de dados e são devidas à exclusão ou modificação de dados acidental ou mal-intencionada, vírus, roubo de equipamentos e sabotagem. CONCLUSÃOTodos precisam ter consciência que os computadores quando interligados, são uma porta aberta para o mundo, com a agravante de não se poder ver quem o está olhando. Quem compartilha um universo tão diversificado, deveria, independentemente de qualquer coisa, prevenir-se contra surpresas desagradáveis. Todos morreremos um dia, uns mais cedo e outros mais tarde. Esta variação de tempo de vida tem muitas influências como: qualidade de vida, localização de moradia, alimentação saudável, prática de esportes, uso de drogas, acidentes, etc. Os sistemas também irão parar de funcionar (crash), esta também é sua tendência natural, devido à influências do meio em que se encontra, má utilização do software, má construção do software (componentes internos, atualizações de versões). Porém isto pode ser prevenido com o auxílio de sistemas antivírus, sistemas firewall (anti-invasão), sistemas de backup, política de segurança, etc. Não haverá nunca um sistema com falha zero, isto é fato. Pela própria natureza humana no mundo real, até onde conhecemos atualmente não existe ser humano imortal, bem como na natureza binária, no mundo digital. A “segurança da informação” é ponto chave para a estabilidade das empresas e continuidade de seus negócios, e desde os primórdios, onde nem se pensava em computação já existia o dito que “prevenir é melhor que remediar”, portanto, a prevenção contra ameaças digitais nunca é demais, quando se trata de usuários domésticos ou principalmente grandes corporações. Este trabalho contribuiu muito para meu desenvolvimento profissional, tive a oportunidade de adquirir conhecimentos que com certeza serão muito úteis na minha vida profissional. BIBLIOGRAFIATHOMAS A. WADLOW, Segurança de Redes - Projeto e gerenciamento de redes seguras Editora Campus - Ano 2000. ROLF JESS FURSTENAU, Programa de Pós graduação em informática na educação (http://penta2.ufrgs.br/edu/tutorialvirus/virus.swf) - Tutorial sobre os vírus. MÓDULO SECURITY, O Protal do Profissional da Segurança da Informação (http://www.modulo.com.br). SYMANTEC CORPORATION; ( http://www.symantec.com). GIORDANI RODRIGUES, InfoGuerra; Segurança e Privacidade (http://www.infoguerra.com.br). SECURITY FOCUS; (http://www.securityfocus.com). REDE NACIONAL DE ENSINO E PESQUISA; (http://www.rnp.br/) EMERSON ALECRIM, Portal Info Wester; (http://www.infowester.com) Download do documento original","link":"/2008/10/23/seguranca-informacao/"}],"tags":[{"name":"OAuth","slug":"OAuth","link":"/tags/OAuth/"},{"name":"Autenticação Centralizada","slug":"Autenticacao-Centralizada","link":"/tags/Autenticacao-Centralizada/"},{"name":"OAuth 2","slug":"OAuth-2","link":"/tags/OAuth-2/"},{"name":"Infraestrutura","slug":"Infraestrutura","link":"/tags/Infraestrutura/"},{"name":"Ldap","slug":"Ldap","link":"/tags/Ldap/"},{"name":"Golang","slug":"Golang","link":"/tags/Golang/"},{"name":"Programação","slug":"Programacao","link":"/tags/Programacao/"},{"name":"Backend","slug":"Backend","link":"/tags/Backend/"},{"name":"Typescript","slug":"Typescript","link":"/tags/Typescript/"},{"name":"Design Patterns","slug":"Design-Patterns","link":"/tags/Design-Patterns/"},{"name":"Adapter Pattern","slug":"Adapter-Pattern","link":"/tags/Adapter-Pattern/"},{"name":"Padrões de Projeto","slug":"Padroes-de-Projeto","link":"/tags/Padroes-de-Projeto/"},{"name":"Frontend","slug":"Frontend","link":"/tags/Frontend/"},{"name":"Cloudflare","slug":"Cloudflare","link":"/tags/Cloudflare/"},{"name":"Github Pages","slug":"Github-Pages","link":"/tags/Github-Pages/"},{"name":"Observer Pattern","slug":"Observer-Pattern","link":"/tags/Observer-Pattern/"},{"name":"Slack","slug":"Slack","link":"/tags/Slack/"},{"name":"Tutoriais","slug":"Tutoriais","link":"/tags/Tutoriais/"},{"name":"Utilitario","slug":"Utilitario","link":"/tags/Utilitario/"},{"name":"PWA","slug":"PWA","link":"/tags/PWA/"},{"name":"Progressive Web Apps","slug":"Progressive-Web-Apps","link":"/tags/Progressive-Web-Apps/"},{"name":"MongoDb","slug":"MongoDb","link":"/tags/MongoDb/"},{"name":"NoSql","slug":"NoSql","link":"/tags/NoSql/"},{"name":"Banco de Dados","slug":"Banco-de-Dados","link":"/tags/Banco-de-Dados/"},{"name":"LDAP","slug":"LDAP","link":"/tags/LDAP/"},{"name":"OpenLdap","slug":"OpenLdap","link":"/tags/OpenLdap/"},{"name":"Aplicativos","slug":"Aplicativos","link":"/tags/Aplicativos/"},{"name":"Deno","slug":"Deno","link":"/tags/Deno/"},{"name":"Npm","slug":"Npm","link":"/tags/Npm/"},{"name":"Node","slug":"Node","link":"/tags/Node/"},{"name":"Digital Ocean","slug":"Digital-Ocean","link":"/tags/Digital-Ocean/"},{"name":"Linux","slug":"Linux","link":"/tags/Linux/"}],"categories":[{"name":"Infraestrutura","slug":"Infraestrutura","link":"/categories/Infraestrutura/"},{"name":"Desenvolvimento","slug":"Desenvolvimento","link":"/categories/Desenvolvimento/"},{"name":"Backend","slug":"Desenvolvimento/Backend","link":"/categories/Desenvolvimento/Backend/"},{"name":"Frontend","slug":"Desenvolvimento/Frontend","link":"/categories/Desenvolvimento/Frontend/"},{"name":"Backend","slug":"Desenvolvimento/Frontend/Backend","link":"/categories/Desenvolvimento/Frontend/Backend/"},{"name":"Segurança","slug":"Infraestrutura/Seguranca","link":"/categories/Infraestrutura/Seguranca/"}]}